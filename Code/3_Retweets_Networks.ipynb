{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retweets Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we create a daily retweet network. First, we transform our dataset into a Source-Target DataFrame. Afterward, we construct a `graph-tool` object.\n",
    "\n",
    "We have 45,330,718 Tweets from 37330 from which 30,918,011 are ReTweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import graph_tool.all as gt\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = \"../../../Data/3_Day_Graphs\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retweet network with a 3-day rolling window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = pd.read_pickle('../../../Data/Tweets_DataFrames/tweets_lite.gzip', compression='gzip')\n",
    "print(f'Cantidad de filas (Tweets): {tweets.shape[0]:,}. Cantidad de Columns: {tweets.shape[1]:,}')\n",
    "print('Total usuarios: ' + f\"{len(set(tweets['Author ID']).union(set(tweets['Referenced Tweet Author ID']))):,}\")\n",
    "tweets.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tenemos: 30,918,011 Retweets y 624,358 Usario con Rts o Retwiteados\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Author ID</th>\n",
       "      <th>Date</th>\n",
       "      <th>Referenced Tweet Author ID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>373097280.0</td>\n",
       "      <td>2021-06-09</td>\n",
       "      <td>12.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>411495598.0</td>\n",
       "      <td>2021-06-10</td>\n",
       "      <td>12.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>411495598.0</td>\n",
       "      <td>2021-06-10</td>\n",
       "      <td>12.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>94589151.0</td>\n",
       "      <td>2021-06-04</td>\n",
       "      <td>12.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>36158466.0</td>\n",
       "      <td>2021-06-04</td>\n",
       "      <td>12.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Author ID        Date  Referenced Tweet Author ID\n",
       "0  373097280.0  2021-06-09                        12.0\n",
       "1  411495598.0  2021-06-10                        12.0\n",
       "2  411495598.0  2021-06-10                        12.0\n",
       "3   94589151.0  2021-06-04                        12.0\n",
       "4   36158466.0  2021-06-04                        12.0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We will just get the Retweets\n",
    "retweets_total = tweets[tweets['Reference Type'] == 'retweeted']\n",
    "retweets_total = retweets_total.drop(columns=['Reference Type'])\n",
    "retweets_total[\"Date\"] = pd.to_datetime(retweets_total[\"Date\"]).dt.date\n",
    "retweets_total = retweets_total.sort_values('Referenced Tweet Author ID').reset_index(drop = True)\n",
    "\n",
    "users = set(retweets_total['Author ID']).union(set(retweets_total['Referenced Tweet Author ID']))\n",
    "print(f'Tenemos: {retweets_total.shape[0]:,} Retweets y {len(users):,} Usario con Rts o Retwiteados')\n",
    "del tweets, users\n",
    "# This will be our Sorce-Target List. Will include the weights of each tweets\n",
    "retweets_total.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Faltan clasificar 587,246 usuarios\n"
     ]
    }
   ],
   "source": [
    "# Import political labelling dictionary\n",
    "user_to_party_paro = pd.read_pickle(\"../../../Data/Pickle/user_to_party_paro.pkl\")\n",
    "\n",
    "ids_faltantes1 = set(retweets_total[\"Author ID\"]) - set(user_to_party_paro.keys())\n",
    "ids_faltantes2 = set(retweets_total[\"Referenced Tweet Author ID\"]) - set(user_to_party_paro.keys())\n",
    "ids_faltantes = np.concatenate((list(ids_faltantes1), list(ids_faltantes2)))\n",
    "print(f\"Faltan clasificar {len(np.unique(ids_faltantes)):,} usuarios\")\n",
    "\n",
    "# Los dejamos como inclasificados \n",
    "for usuario in ids_faltantes:\n",
    "    user_to_party_paro[usuario] = np.nan\n",
    "\n",
    "df = pd.DataFrame(list(user_to_party_paro.items()), columns=['User ID', 'Political Affiliation'])\n",
    "df['Political Affiliation'].value_counts(dropna=False)\n",
    "del df, ids_faltantes, ids_faltantes1, ids_faltantes2, usuario"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cantidad de filas (Tweets): 13,784,630. Cantidad de Columns: 4\n",
      "Tenemos: 13,784,630 Retweets y 36,964 Usario con Rts o Retwiteados\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Author ID</th>\n",
       "      <th>Date</th>\n",
       "      <th>Referenced Tweet Author ID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>373097280.0</td>\n",
       "      <td>2021-06-09</td>\n",
       "      <td>12.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>373097280.0</td>\n",
       "      <td>2021-06-09</td>\n",
       "      <td>12.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>373097280.0</td>\n",
       "      <td>2021-06-10</td>\n",
       "      <td>12.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>373097280.0</td>\n",
       "      <td>2021-06-10</td>\n",
       "      <td>12.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>411495598.0</td>\n",
       "      <td>2021-06-10</td>\n",
       "      <td>12.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Author ID        Date  Referenced Tweet Author ID\n",
       "0  373097280.0  2021-06-09                        12.0\n",
       "1  373097280.0  2021-06-09                        12.0\n",
       "2  373097280.0  2021-06-10                        12.0\n",
       "3  373097280.0  2021-06-10                        12.0\n",
       "4  411495598.0  2021-06-10                        12.0"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Importamos master ids para filtrar\n",
    "# Bring Master ID file\n",
    "users_information = pd.read_pickle('/mnt/disk2/Data/Tweets_DataFrames/users_information.gzip', compression='gzip')\n",
    "master_id = pd.DataFrame(users_information.index.get_level_values('Author ID'))\n",
    "\n",
    "# Filter by master Id\n",
    "temp = retweets_total.merge(master_id, how = 'inner', on = 'Author ID')\n",
    "retweets = temp.merge(master_id, how = 'inner', left_on = 'Referenced Tweet Author ID', right_on='Author ID')\n",
    "print(f'Cantidad de filas (Tweets): {retweets.shape[0]:,}. Cantidad de Columns: {retweets.shape[1]:,}')\n",
    "retweets = retweets.drop(columns= 'Author ID_y')\n",
    "retweets = retweets.rename(columns={'Author ID_x': 'Author ID'})\n",
    "\n",
    "# Count how many tweets and users we have now\n",
    "users = set(list(retweets['Author ID']).union(set(retweets['Referenced Tweet Author ID'])))\n",
    "print(f'Tenemos: {retweets.shape[0]:,} Retweets y {len(users):,} Usario con Rts o Retwiteados')\n",
    "retweets.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nodos Retweeteados por alguien pero que no Retweetean: 6,510\n",
      "Nodos que Retweetean a alguien pero no son Retweeteados: 833\n",
      "Nodos que tienen un self loop: 3\n",
      "primer nodo: 12.0 último nodo: 1.3897841454176788e+18\n",
      "Total de nodos en la muestra: 36,964\n"
     ]
    }
   ],
   "source": [
    "nodes_no_receipt = set(retweets[\"Author ID\"]) -  set(retweets[\"Referenced Tweet Author ID\"])\n",
    "nodes_no_send = set(retweets[\"Referenced Tweet Author ID\"]) - set(retweets[\"Author ID\"])\n",
    "self_loops = set(retweets[retweets['Author ID'] == retweets['Referenced Tweet Author ID']])\n",
    "\n",
    "print(f'Nodos Retweeteados por alguien pero que no Retweetean: {len(nodes_no_receipt):,}')\n",
    "print(f'Nodos que Retweetean a alguien pero no son Retweeteados: {len(nodes_no_send):,}')\n",
    "print(f'Nodos que tienen un self loop: {len(self_loops)}')\n",
    "\n",
    "nodes_id = list(set(retweets[\"Author ID\"]).union(set(retweets[\"Referenced Tweet Author ID\"])))\n",
    "nodes_id.sort()\n",
    "print(f'primer nodo: {nodes_id[0]} último nodo: {nodes_id[-1]}')\n",
    "print(f'Total de nodos en la muestra: {len(nodes_id):,}')\n",
    "\n",
    "del nodes_no_receipt, nodes_no_send, self_loops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 36964/36964 [00:00<00:00, 1589760.91it/s]\n"
     ]
    }
   ],
   "source": [
    "nodes_political_affilitation = []\n",
    "\n",
    "for i in tqdm(nodes_id):\n",
    "    # Assign Political Affiliation\n",
    "    pa = user_to_party_paro.get(i, np.nan) # using .get() for safe dictionary lookup\n",
    "    nodes_political_affilitation.append(pa)\n",
    "\n",
    "num_nodes = len(nodes_id)\n",
    "nodes_idx = [i for i in range(num_nodes)]\n",
    "\n",
    "# Create dictionary\n",
    "nodes_dict = {\n",
    "    'id': nodes_id,\n",
    "    'idx': nodes_idx,\n",
    "    'pa': nodes_political_affilitation,\n",
    "}\n",
    "\n",
    "# Save as pickle\n",
    "with open(\"../../../Data/3_Day_Graphs/nodes_dict.pkl\", \"wb\") as f:\n",
    "    pickle.dump(nodes_dict, f)\n",
    "\n",
    "del i, num_nodes, pa, f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dates of the Paro Nacional\n",
    "v1_start = '2021-04-28 00:00:00'\n",
    "v1_end = '2021-06-27 00:00:00'\n",
    "date_start = pd.date_range(start = v1_start, end = v1_end, freq = 'D')\n",
    "\n",
    "del v1_end, v1_start\n",
    "\n",
    "v2_start = '2021-04-30 23:59:59'\n",
    "v2_end = '2021-06-29 23:59:59'\n",
    "date_end = pd.date_range(start = v2_start, end = v2_end, freq = 'D')\n",
    "\n",
    "del v2_start, v2_end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "61it [06:09,  6.06s/it]\n"
     ]
    }
   ],
   "source": [
    "# Create a dictionary for O(1) lookup time of indices\n",
    "id_to_idx = {node_id: idx for idx, node_id in enumerate(nodes_dict['id'])}\n",
    "id_to_idx\n",
    "\n",
    "# Runtime 1 Hour!!!!!\n",
    "for d_s, d_e in tqdm(zip(date_start, date_end)):\n",
    "    # Get 3 days Retweets\n",
    "    window = retweets[(retweets['Date'] >= d_s.date()) & (retweets['Date'] <= d_e.date())] \n",
    "\n",
    "    # Get total of Tweets during that period\n",
    "    rts_per_user= window.groupby('Author ID').size().reset_index(name = 'total')\n",
    "\n",
    "    # Create Normalize Weight\n",
    "    temp = window.groupby([\"Author ID\", \"Referenced Tweet Author ID\"]).size().reset_index(name = \"number of rts\")\n",
    "    temp = temp.merge(rts_per_user, how = 'right', on='Author ID')\n",
    "    temp['normal weight'] = temp['number of rts']/temp['total']\n",
    "    temp.columns = [\"source\", \"target\", \"number of rts\", \"total\", \"normal weight\"]\n",
    "\n",
    "    # Assign political affiliation\n",
    "    temp[\"source_political_afilliation\"] = temp[\"source\"].apply(lambda x: user_to_party_paro[x])\n",
    "    temp[\"target_political_afilliation\"] = temp[\"target\"].apply(lambda x: user_to_party_paro[x])\n",
    "    temp.dropna(subset=['target_political_afilliation', 'source_political_afilliation'], inplace=True)\n",
    "        \n",
    "    # Save results as csv (1 min. each aprox)\n",
    "    temp.to_csv(os.path.join(save_path, \"Source-Target\", str(d_e.date()) + \".csv\"), index = False, sep = \";\")\n",
    "\n",
    "del d_s, d_e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Numero de Arcos: 466,447. Nodos Source: 28,403. Nodos Target 12,584\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source</th>\n",
       "      <th>target</th>\n",
       "      <th>number of rts</th>\n",
       "      <th>total</th>\n",
       "      <th>normal weight</th>\n",
       "      <th>source_political_afilliation</th>\n",
       "      <th>target_political_afilliation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>777978.0</td>\n",
       "      <td>135629634.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>Retweets Centro</td>\n",
       "      <td>No Retweets</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>784125.0</td>\n",
       "      <td>58956408.0</td>\n",
       "      <td>1</td>\n",
       "      <td>22</td>\n",
       "      <td>0.045455</td>\n",
       "      <td>Retweets Izquierda</td>\n",
       "      <td>Retweets Centro</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>784125.0</td>\n",
       "      <td>61028380.0</td>\n",
       "      <td>1</td>\n",
       "      <td>22</td>\n",
       "      <td>0.045455</td>\n",
       "      <td>Retweets Izquierda</td>\n",
       "      <td>Retweets Izquierda</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>784125.0</td>\n",
       "      <td>82531058.0</td>\n",
       "      <td>1</td>\n",
       "      <td>22</td>\n",
       "      <td>0.045455</td>\n",
       "      <td>Retweets Izquierda</td>\n",
       "      <td>Retweets Izquierda</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>784125.0</td>\n",
       "      <td>108371496.0</td>\n",
       "      <td>1</td>\n",
       "      <td>22</td>\n",
       "      <td>0.045455</td>\n",
       "      <td>Retweets Izquierda</td>\n",
       "      <td>Retweets Izquierda</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     source       target  number of rts  total  normal weight  \\\n",
       "0  777978.0  135629634.0              1      1       1.000000   \n",
       "1  784125.0   58956408.0              1     22       0.045455   \n",
       "2  784125.0   61028380.0              1     22       0.045455   \n",
       "3  784125.0   82531058.0              1     22       0.045455   \n",
       "4  784125.0  108371496.0              1     22       0.045455   \n",
       "\n",
       "  source_political_afilliation target_political_afilliation  \n",
       "0              Retweets Centro                  No Retweets  \n",
       "1           Retweets Izquierda              Retweets Centro  \n",
       "2           Retweets Izquierda           Retweets Izquierda  \n",
       "3           Retweets Izquierda           Retweets Izquierda  \n",
       "4           Retweets Izquierda           Retweets Izquierda  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example of our lists\n",
    "save_path = \"../../../Data/3_Day_Graphs\"\n",
    "ej_csv = pd.read_csv(save_path + '/Source-Target/2021-04-30.csv', sep = ';')\n",
    "ej_csv.dropna(inplace=True)\n",
    "print(f\" Numero de Arcos: {ej_csv.shape[0]:,}. Nodos Source: {len(ej_csv.source.unique()):,}. Nodos Target {len(ej_csv.target.unique()):,}\")\n",
    "ej_csv.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def partir_y_consultar(lista,k):\n",
    "    import math\n",
    "    concat = []\n",
    "\n",
    "    N = len(lista)\n",
    "    parte = math.floor(N/k)\n",
    "    slices = [i for i in range(0,N+1,parte)]\n",
    "    for i in range(len(slices)-1):\n",
    "        if slices[i+1] == slices[-1]:\n",
    "            sub = lista[slices[i]:]\n",
    "        else:\n",
    "            sub = lista[slices[i]:slices[i+1]]\n",
    "        concat.append(sub)\n",
    "        \n",
    "    return concat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/61 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FASE 1 CHECK\n",
      "FASE 2 CHECK\n",
      "FASE 3.1 CHECK\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/61 [01:07<?, ?it/s]\n"
     ]
    },
    {
     "ename": "MemoryError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m/mnt/disk2/fcastrillon/Analysis-of-Tweets-During-the-2021-Social-Unrest/Code/3_Retweets_Networks.ipynb Cell 16\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B157.253.242.110/mnt/disk2/fcastrillon/Analysis-of-Tweets-During-the-2021-Social-Unrest/Code/3_Retweets_Networks.ipynb#X65sdnNjb2RlLXJlbW90ZQ%3D%3D?line=16'>17</a>\u001b[0m source_target \u001b[39m=\u001b[39m [row \u001b[39mfor\u001b[39;00m row \u001b[39min\u001b[39;00m temp[[\u001b[39m'\u001b[39m\u001b[39msource\u001b[39m\u001b[39m'\u001b[39m,\u001b[39m'\u001b[39m\u001b[39mtarget\u001b[39m\u001b[39m'\u001b[39m]]\u001b[39m.\u001b[39mitertuples(name \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m, index\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)]\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B157.253.242.110/mnt/disk2/fcastrillon/Analysis-of-Tweets-During-the-2021-Social-Unrest/Code/3_Retweets_Networks.ipynb#X65sdnNjb2RlLXJlbW90ZQ%3D%3D?line=17'>18</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mFASE 3.1 CHECK\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2B157.253.242.110/mnt/disk2/fcastrillon/Analysis-of-Tweets-During-the-2021-Social-Unrest/Code/3_Retweets_Networks.ipynb#X65sdnNjb2RlLXJlbW90ZQ%3D%3D?line=18'>19</a>\u001b[0m g\u001b[39m.\u001b[39;49madd_edge_list(source_target)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B157.253.242.110/mnt/disk2/fcastrillon/Analysis-of-Tweets-During-the-2021-Social-Unrest/Code/3_Retweets_Networks.ipynb#X65sdnNjb2RlLXJlbW90ZQ%3D%3D?line=19'>20</a>\u001b[0m \u001b[39m# partition = partir_y_consultar(source_target, 200000)\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B157.253.242.110/mnt/disk2/fcastrillon/Analysis-of-Tweets-During-the-2021-Social-Unrest/Code/3_Retweets_Networks.ipynb#X65sdnNjb2RlLXJlbW90ZQ%3D%3D?line=20'>21</a>\u001b[0m \u001b[39m# print('FASE 3.2 CHECK')\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B157.253.242.110/mnt/disk2/fcastrillon/Analysis-of-Tweets-During-the-2021-Social-Unrest/Code/3_Retweets_Networks.ipynb#X65sdnNjb2RlLXJlbW90ZQ%3D%3D?line=21'>22</a>\u001b[0m \u001b[39m# del source_target\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B157.253.242.110/mnt/disk2/fcastrillon/Analysis-of-Tweets-During-the-2021-Social-Unrest/Code/3_Retweets_Networks.ipynb#X65sdnNjb2RlLXJlbW90ZQ%3D%3D?line=33'>34</a>\u001b[0m \u001b[39m#         g.add_edge_list(i)\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B157.253.242.110/mnt/disk2/fcastrillon/Analysis-of-Tweets-During-the-2021-Social-Unrest/Code/3_Retweets_Networks.ipynb#X65sdnNjb2RlLXJlbW90ZQ%3D%3D?line=34'>35</a>\u001b[0m \u001b[39m# del i,x,partition\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B157.253.242.110/mnt/disk2/fcastrillon/Analysis-of-Tweets-During-the-2021-Social-Unrest/Code/3_Retweets_Networks.ipynb#X65sdnNjb2RlLXJlbW90ZQ%3D%3D?line=36'>37</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mFASE 3.3 CHECK\u001b[39m\u001b[39m'\u001b[39m)\n",
      "File \u001b[0;32m~/.conda/envs/gt/lib/python3.11/site-packages/graph_tool/__init__.py:2650\u001b[0m, in \u001b[0;36mGraph.add_edge_list\u001b[0;34m(self, edge_list, hashed, hash_type, eprops)\u001b[0m\n\u001b[1;32m   2648\u001b[0m         libcore\u001b[39m.\u001b[39madd_edge_list(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m__graph, edge_list, eprops)\n\u001b[1;32m   2649\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 2650\u001b[0m         libcore\u001b[39m.\u001b[39;49madd_edge_list_iter(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m__graph, edge_list, eprops)\n\u001b[1;32m   2651\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   2652\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(edge_list, numpy\u001b[39m.\u001b[39mndarray):\n",
      "\u001b[0;31mMemoryError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Create Graph\n",
    "files = glob.glob('../../../Data/3_Day_Graphs/Source-Target/*.csv')\n",
    "\n",
    "# Open dict for any case\n",
    "with open('/mnt/disk2/Data/3_Day_Graphs/nodes_dict.pkl', 'rb') as file:\n",
    "    nodes_dict = pickle.load(file)\n",
    "\n",
    "for file in tqdm(files):\n",
    "    # Read Graph\n",
    "    date = file.split('/')[-1].split('.')[0]\n",
    "    temp = pd.read_csv(file, sep = ';')\n",
    "    print('FASE 1 CHECK')\n",
    "    # Initialice Directed Graph\n",
    "    g = gt.Graph(directed = True)\n",
    "    print('FASE 2 CHECK')\n",
    "    # Create Source Target Partition\n",
    "    source_target = [row for row in temp[['source','target']].itertuples(name = None, index=False)]\n",
    "    print('FASE 3.1 CHECK')\n",
    "    g.add_edge_list(source_target)\n",
    "    # partition = partir_y_consultar(source_target, 200000)\n",
    "    # print('FASE 3.2 CHECK')\n",
    "    # del source_target\n",
    "    \n",
    "    # x = 1\n",
    "    # n = 0\n",
    "    # # Add source Target by peaces\n",
    "    # for i in partition:\n",
    "    #     if x < 10:\n",
    "    #         g.add_edge_list(i)\n",
    "    #         print(f\"Termina iteración {x}\")\n",
    "    #         print()\n",
    "    #         x+=1\n",
    "    #     else:\n",
    "    #         g.add_edge_list(i)\n",
    "    # del i,x,partition\n",
    "    \n",
    "    print('FASE 3.3 CHECK')\n",
    "    # Create an edge property map for weights\n",
    "    edge_weight_map = g.new_edge_property(\"double\")\n",
    "\n",
    "    # Create a Vertex property map for labels\n",
    "    vertex_label_map = g.new_vertex_property('string')\n",
    "\n",
    "    # Assign weights to the edges using a for loop\n",
    "    edge_list = g.get_edges()\n",
    "    weights = list(temp['normal weight'])\n",
    "\n",
    "    for i, e in enumerate(edge_list):\n",
    "        #print(f'Property of {e} is {weights[i]}. Type of Edge: {type(e)}')\n",
    "        edge_weight_map[e] = weights[i]\n",
    "    \n",
    "    del edge_list, weights\n",
    "\n",
    "    # Assign Labels to vertices using loop\n",
    "    vertex_list = g.get_vertices()\n",
    "    labels = nodes_dict['pa']\n",
    "    for i,v in enumerate(vertex_list):\n",
    "        print(i,v)\n",
    "        print(f'Property of {v} is {labels[i]}. Type of Edge: {type(v)}')\n",
    "        label_of_v = nodes_dict['id'].index[v]\n",
    "        vertex_label_map[v] = labels[label_of_v]\n",
    "    \n",
    "    del vertex_list, labels\n",
    "\n",
    "    # Add properties\n",
    "    g.vp['Political Label'] = vertex_label_map\n",
    "    g.ep['Normal Weights'] = edge_weight_map\n",
    "\n",
    "    # Save Graphs\n",
    "    filename = os.path.join(save_path, date + \".graphml\")\n",
    "    g.save(filename)\n",
    "    print(f\"Successfully Saved file {filename} SIIIIIUUUUU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of our graphs\n",
    "ej_g = gt.load_graph(save_path + f'/{str(date_end[0].date())}.graphml')\n",
    "ej_g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import graph_tool.all as gt\n",
    "\n",
    "# Create a graph\n",
    "g = gt.Graph(directed=False)\n",
    "v1 = g.add_vertex()\n",
    "v2 = g.add_vertex()\n",
    "v3 = g.add_vertex()\n",
    "v4 = g.add_vertex()\n",
    "\n",
    "# Add edges\n",
    "e1 = g.add_edge(v1, v2)\n",
    "e2 = g.add_edge(v2, v3)\n",
    "e3 = g.add_edge(v3, v4)\n",
    "\n",
    "# Create an edge property map for weights\n",
    "edge_weight_map = g.new_edge_property(\"double\")\n",
    "\n",
    "# Create a Vertex property map for labels\n",
    "vertex_label_map = g.new_vertex_property('string')\n",
    "\n",
    "# Assign weights to the edges using a for loop\n",
    "edge_list = g.get_edges()\n",
    "weights = [5.0, 3.2, 7.1]  # Example weights\n",
    "for i, e in enumerate(edge_list):\n",
    "    print(f'Property of {e} is {weights[i]}. Type of Edge: {type(e)}')\n",
    "    edge_weight_map[e] = weights[i]\n",
    "\n",
    "# Assign Labels to vertices using loop\n",
    "vertex_list = g.get_vertices()\n",
    "labels = ['par', 'impar', 'par', 'impar']\n",
    "for i,v in enumerate(vertex_list):\n",
    "    print(i,v)\n",
    "    print(f'Property of {v} is {labels[i]}. Type of Edge: {type(v)}')\n",
    "    vertex_label_map[v] = labels[i]\n",
    "\n",
    "# Access and print the weights for each edge\n",
    "for v in g.vertices():\n",
    "    print(f\"Label of Vertex {v}: {vertex_label_map[v]}\")\n",
    "\n",
    "g.vp['Label'] = vertex_label_map\n",
    "g.vp['Label']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Daily retweet network (with no rolling window)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_paro = glob.glob('/mnt/disk2/Data/Tweets_DataFrames/tweets_paro_*')\n",
    "tweets_paro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the tweets from Paro but select only the retweets\n",
    "retweets = pd.DataFrame()\n",
    "for file in tqdm(tweets_paro):\n",
    "    tweets = pd.read_pickle(file, compression = \"gzip\")\n",
    "\n",
    "    # Select only retweets\n",
    "    rts = tweets.loc[tweets[\"Reference Type\"] == \"retweeted\",:].reset_index(drop = True)\n",
    "    rts = rts.drop(columns = 'Reference Type')\n",
    "    retweets = pd.concat([retweets, rts], axis = 0)\n",
    "retweets = retweets.reset_index(drop = True)\n",
    "del rts, tweets \n",
    "print('Shape:', retweets.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retweets.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix data types\n",
    "retweets[\"ID\"] = retweets[\"ID\"].astype(int)\n",
    "retweets[\"Author ID\"] = retweets[\"Author ID\"].astype(int)\n",
    "retweets['Referenced Tweet Author ID'] = retweets['Referenced Tweet Author ID'].astype(int)\n",
    "retweets['Referenced Tweet'] = retweets['Referenced Tweet'].astype(int)\n",
    "\n",
    "# Remove time from retweets date\n",
    "retweets[\"Date\"] = pd.to_datetime(retweets[\"Date\"]).dt.date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import political labelling dictionary\n",
    "user_to_party_paro = pd.read_pickle(\"/mnt/disk2/Data/Pickle/user_to_party_paro.pkl\")\n",
    "\n",
    "ids_faltantes1 = set(retweets[\"Author ID\"]) - set(user_to_party_paro.keys())\n",
    "ids_faltantes2 = set(retweets[\"Referenced Tweet Author ID\"]) - set(user_to_party_paro.keys())\n",
    "ids_faltantes = np.concatenate((list(ids_faltantes1), list(ids_faltantes2)))\n",
    "print(f\"Faltan clasificar {len(np.unique(ids_faltantes))} usuarios\")\n",
    "\n",
    "# Los dejamos como inclasificados \n",
    "for usuario in ids_faltantes:\n",
    "    user_to_party_paro[usuario] = np.nan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Source-Target DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = \"../../Data/Daily_graphs\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fecha_min = retweets[\"Date\"].min()\n",
    "fecha_max = retweets[\"Date\"].max()\n",
    "print(f\"We have retweets from {fecha_min.strftime('%Y-%m-%d')} to {fecha_max.strftime('%Y-%m-%d')}\")\n",
    "daily_grid = pd.date_range(start = fecha_min, end = fecha_max, freq = 'D')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "users_information = pd.read_pickle(\"../../../Data/Tweets_DataFrames/users_information.gzip\", compression = \"gzip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "users_information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Label Referenced Tweet Author\n",
    "retweets = users_information.reset_index()[[\"Author ID\", \"Author Name\"]] \\\n",
    "    .rename(columns = {\"Author ID\": \"Referenced Tweet Author ID\", \n",
    "                       \"Author Name\": \"Referenced Tweet Author Name\"}) \\\n",
    "                       .merge(retweets, how = \"right\", on = \"Referenced Tweet Author ID\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retweets.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Users without name\n",
    "retweets.iloc[:, 0:2].drop_duplicates().isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for d in tqdm(daily_grid):\n",
    "    # Select the retweets from the desired date\n",
    "    temp = retweets[retweets['Date'] == d.date()]\n",
    "    temp = temp.groupby([\"Author ID\", \"Author Name\", \"Date\", \"Referenced Tweet Author ID\", \"Referenced Tweet Author Name\"]).size().reset_index(name = \"w\")\n",
    "    temp.columns = [\"source\", \"source_label\", \"date\", \"target\", \"target_label\", \"w\"]\n",
    "    # Assign political affiliation\n",
    "    temp[\"source_political_afilliation\"] = temp[\"source\"].apply(lambda x: user_to_party_paro[x])\n",
    "    temp[\"target_political_afilliation\"] = temp[\"target\"].apply(lambda x: user_to_party_paro[x])\n",
    "    # Save results as csv\n",
    "    temp.to_csv(os.path.join(save_path, \"Source-Target\", str(d.date()) + \".csv\"), index = False, sep = \";\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Node list\n",
    "nodes_id = list(set(retweets[\"Author ID\"]).union(set(retweets[\"Referenced Tweet Author ID\"])))\n",
    "len(nodes_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "users_information.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "users_information = users_information.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert users_information DataFrame to a dictionary for fast lookup\n",
    "users_info_dict = users_information.set_index('Author ID')['Author Name'].to_dict()\n",
    "\n",
    "nodes_label = []\n",
    "nodes_political_affilitation = []\n",
    "for i in tqdm(nodes_id):\n",
    "    # Assign label\n",
    "    label = users_info_dict.get(i, np.nan) # using .get() for safe dictionary lookup\n",
    "    nodes_label.append(label)\n",
    "\n",
    "    # Assign Political Affiliation\n",
    "    pa = user_to_party_paro.get(i, np.nan) # using .get() for safe dictionary lookup\n",
    "    nodes_political_affilitation.append(pa)\n",
    "\n",
    "num_nodes = len(nodes_id)\n",
    "nodes_idx = [i for i in range(num_nodes)]\n",
    "\n",
    "# Create dictionary\n",
    "nodes_dict = {\n",
    "    'id': nodes_id,\n",
    "    'idx': nodes_idx,\n",
    "    'label': nodes_label,\n",
    "    'pa': nodes_political_affilitation\n",
    "}\n",
    "\n",
    "# Save as pickle\n",
    "with open(\"../../../Data/Daily_graphs/nodes_dict.pkl\", \"wb\") as f:\n",
    "    pickle.dump(nodes_dict, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary for O(1) lookup time of indices\n",
    "id_to_idx = {node_id: idx for idx, node_id in enumerate(nodes_dict['id'])}\n",
    "\n",
    "for d in tqdm(daily_grid):\n",
    "    # Filter retweets first\n",
    "    filtered_retweets = retweets[retweets['Date'] == d.date()]\n",
    "    \n",
    "    # Create weights\n",
    "    test = filtered_retweets.groupby(['Author ID', 'Referenced Tweet Author ID']).size().reset_index(name=\"w\")\n",
    "    \n",
    "    # Use dictionary lookups for index mappings\n",
    "    test['idx_s'] = [id_to_idx.get(x, np.nan) for x in test['Author ID']]\n",
    "    test['idx_t'] = [id_to_idx.get(x, np.nan) for x in test['Referenced Tweet Author ID']]\n",
    "    \n",
    "    # The edge list should be based on the idx of the nodes and include weights\n",
    "    edges_list_with_weights = list(zip(test['idx_s'].dropna().astype(int), test['idx_t'].dropna().astype(int), test['w']))\n",
    "    \n",
    "    # Create graph\n",
    "    g = gt.Graph(directed=True)\n",
    "    g.add_vertex(num_nodes)\n",
    "    \n",
    "    # Node attributes\n",
    "    node_id = g.new_vertex_property(\"string\", vals=nodes_dict['id'])\n",
    "    node_label = g.new_vertex_property(\"string\", vals=nodes_dict['label'])\n",
    "    node_affiliation = g.new_vertex_property(\"string\", vals=nodes_dict['pa'])\n",
    "    \n",
    "    # Assign attributes to graph\n",
    "    g.vertex_properties[\"ID\"] = node_id\n",
    "    g.vertex_properties[\"Label\"] = node_label\n",
    "    g.vertex_properties[\"Political Affiliation\"] = node_affiliation\n",
    "    \n",
    "    # Edge weight attribute\n",
    "    edge_weights = g.new_edge_property(\"int\")\n",
    "    \n",
    "    # Add edges and assign weights\n",
    "    for source, target, weight in edges_list_with_weights:\n",
    "        e = g.add_edge(source, target)\n",
    "        edge_weights[e] = weight\n",
    "\n",
    "    g.edge_properties[\"weight\"] = edge_weights\n",
    "    \n",
    "    g.save(os.path.join(save_path, str(d.date()) + \".graphml\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(g)\n",
    "print(g.vp[\"ID\"][624353])\n",
    "print(g.vp[\"Label\"][624353])\n",
    "print(g.vp[\"Political Affiliation\"][624353])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
