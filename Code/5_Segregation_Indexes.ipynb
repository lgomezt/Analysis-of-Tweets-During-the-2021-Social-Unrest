{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Segregation Indexes\n",
    "Following the Bojanowski & Corten Paper _Measuring Segregation in Social Networks_ 2014, We calculate some segregation indexes for our graphs along the time. First, we import the data and libraries ised for the creation of the functions in the Prerequiste section. Then We calculate The Freeman Segregation Index and the Spectral Segregatoin Index in The next Sections. This Notebook is divided en the following sections.\n",
    "\n",
    "1. Prerequisites\n",
    "2. Freeman Segregation\n",
    "\t- Basic Freeman Segregation\n",
    "\t- Global Freeman Segregation Index (for K groups)\n",
    "\t- Freeman Segregation Index for an specific group\n",
    "\t- Freeman Segregation Index for an specific group (Taking in account Weights)\n",
    "3. Assortativity\n",
    "4. Results\n",
    "4.1. 3 Day Rolling Window\n",
    "4. Conclusion\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Prerequistes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/disk2/anaconda3/envs/gt_global/lib/python3.11/site-packages/graph_tool/draw/cairo_draw.py:1544: RuntimeWarning: Error importing Gtk module: ; GTK+ drawing will not work.\n",
      "  warnings.warn(msg, RuntimeWarning)\n"
     ]
    }
   ],
   "source": [
    "# Mathematical and Data Managment\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Graph Managment\n",
    "import graph_tool.all as gt\n",
    "import utils.Freeman as Fr\n",
    "import utils.Proximity as Pr\n",
    "import utils.Homophily as Ho\n",
    "\n",
    "# Miscellaneous\n",
    "from glob import glob\n",
    "from tqdm import tqdm\n",
    "import concurrent.futures\n",
    "from functools import partial\n",
    "from time import perf_counter\n",
    "import os\n",
    "import re\n",
    "\n",
    "# Paths\n",
    "path = r\"/mnt/disk2/Data\"\n",
    "path_3_day = os.path.join(path,\"3_Day_Graphs\")\n",
    "path_daily = os.path.join(path,\"Daily_Graphs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the calculation of the segregation Indexes, we define some notation based on (Bojanowski & Corten 2014).\n",
    "\n",
    "We define a Graph with\n",
    "\n",
    "$$\\mathbb{N}= \\{1, \\dots, i, \\dots, N\\}$$\n",
    "\n",
    "and then, define the set\n",
    "\n",
    "$$\\mathbb{G} = \\{G_1, G_2,\\dots, G_k\\}$$\n",
    "\n",
    "as the set of $K$ groups in which every $G_g$ is a subset of $\\mathbb{N}$ that contains all the nodes that belong to group $g$. define $\\eta_{k}$ = $|G_k|$ as the amount of nodes from group $G_k$\n",
    "\n",
    "Now we define the type vector as \n",
    "\n",
    "$$\\textbf{t} = [t_1,\\dots, t_i, \\dots, t_N]$$\n",
    "\n",
    "where $t_i \\in \\{1,\\dots,K\\}$. This vector matches every node with its corresponding group. Using this notation. We can define a type indicator vector for each group $k$ as follows:  \n",
    "\n",
    "$$\\textbf{v}_k = [v_1, \\dots, v_i, \\dots, v_N]$$ \n",
    "\n",
    "where $v_i \\in \\{0,1\\}$. This vector has one entry for every node and the node location will have a 1 if that node corresponds to the group $G_k$. Formally:\n",
    "\n",
    "$$ v_i = \\begin{cases} 1 &\\text{ if }t_i = k \\\\ 0 &\\text{ if }t_i \\neq k \\end{cases} $$\n",
    "\n",
    "Now we define the Types Matrix $T_{N\\times K}$ as a matrix that contains the information of each node and wich group it represents. For Every column of the matrix corresponds to a $\\textbf{v}_k$ _types indicator vector_.\n",
    "\n",
    "In the context of this Research, we will use a Directed Weighted Graph. Our nodes are X (formmerly Twitter) in which the user $i$ is related to the user $j$ if $i$ Retweeted a Tweet without comments of $j$. In this sense. Formarly we discribe the relationship $R$ over $\\mathbb{N}\\times \\mathbb{N}$ that implies our square Adjcency Matrix $X = [X_{ij}]_{\\mathbb{N}\\times \\mathbb{N}}$\n",
    "\n",
    "For the segregation calculations will will consider the graph as weighted or un weight. In case of takin in account the weights of each edge, eh entries of the Adjacency Matrix will be defined as follows:\n",
    "\n",
    "$$x_{ij} = \\dfrac{\\text{\\# Tweets from }j\\text{ that }i\\text{ Retweeted without comments}}{\\text{\\# of Retweets without comments from }i}$$\n",
    "\n",
    "Taking in account the unweighted graph, we will define our Simple Adjacency Matrix as:\n",
    "\n",
    "$$ x_{ij} = \\begin{cases} 1 & \\text{if } i\\text{ Retweeted }j \\\\ 0 & \\text{In other case} \\end{cases} $$\n",
    "\n",
    "Finally, we define the _Mixing Matrix_ ($M_{ghy}$) where $g$ and $h$ are two generic groups and $y$ indexes two types of layers. The first layer _Mixing Matrix_ is the _Contact layer_, defined as follows: (Where we use the weighted or un weighted adjcancecy matrix):\n",
    "\n",
    "$$M_{gh1} = \\sum_{i\\in G_g}\\sum_{j\\in G_h} x_{ij}$$\n",
    "\n",
    "For the **unweighted** case, we can define the _No Contact Layer_ as follows:\n",
    "\n",
    "$$M_{gh0} = \\sum_{i\\in G_g}\\sum_{j\\in G_h} (1-x_{ij})$$\n",
    "\n",
    "Finally, in this matrix $M_{gh1}$ shows the amount of attention that group $h$ gets from group $g$\n",
    "\n",
    "For easyness, we define the follow notation:\n",
    "\n",
    "- $M_{g+1} = \\sum_{h=1}^K M_{gh1}$ Sum across a column\n",
    "\n",
    "- $M_{+h1} = \\sum_{g=1}^K M_{gh1}$ Sum across a row\n",
    "\n",
    "- $M_{++1} = \\sum_{g=1}^K \\sum_{h=1}^K M_{gh1}$ Sum of all the Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "master_id = pd.read_csv(os.path.join(path,\"Master_Index.csv\"), sep = ';')\n",
    "\n",
    "# Indexes\n",
    "date_range_daily = pd.date_range(start='2021-04-28', end='2021-06-29', freq='D')\n",
    "date_range_3day = pd.date_range(start='2021-04-28', end='2021-06-27', freq='D')\n",
    "categories = master_id['Political Affiliation'].unique().tolist()\n",
    "\n",
    "group_index_3day = pd.MultiIndex.from_product([date_range_3day, categories], names=['Date', 'Political Label'])\n",
    "group_index_daily = pd.MultiIndex.from_product([date_range_3day, categories], names=['Date', 'Political Label'])\n",
    "\n",
    "individual_index = pd.MultiIndex.from_product([range(0,len(master_id)), categories], names=['Node', 'Political Label'])\n",
    "\n",
    "# 3 Day DataFrames\n",
    "global_segregation_3day = pd.DataFrame(index=date_range_3day).sort_index()\n",
    "group_segregation_3day = pd.DataFrame(index=group_index_3day).sort_index()\n",
    "\n",
    "# Daily Dataframes\n",
    "global_segregation_daily = pd.DataFrame(index=date_range_daily).sort_index()\n",
    "group_segregation_daily = pd.DataFrame(index=group_index_daily).sort_index()\n",
    "\n",
    "# Individual Dataframes\n",
    "individual_group_segregation = pd.DataFrame(index=individual_index).sort_index()\n",
    "individual_node_segregation = master_id[['Political Affiliation']].rename(columns = {'Political Affiliation': 'Political Label'})\n",
    "\n",
    "# Load graphs\n",
    "os.path.join(path_3_day,\"Graphs\", \"*.graphml\")\n",
    "files_3day = glob(os.path.join(path_3_day,\"Graphs\", \"*.graphml\"))\n",
    "files_daily = glob(os.path.join(path_daily,\"Graphs\", \"*.graphml\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 2. Freman Segregation\n",
    "\n",
    "### Basic Freeman Segregation\n",
    "\n",
    "The basic segregation index proposed by Freeman (1998) tries to see how is the proportion of ties between two different groups against the ties if they were made randomly. This Basic Index is calculated for undirected and unweighted graphs. It is the first approch for segregation for this family of indexes. We define the $\\eta_1$ as the amount of nodes belonng to group $G_1$ and $\\eta_2$ belonging to group $G_2$. Recalling our past notation, in the contact Layer of the mixing matrix the number of between group ties will be the entry $M_{121}$. We can divide this numero over all the amount of edges (Note that the nuber of edges in the graph is equal to the sum of all the contact layer $M_{++1}$). This is how we get the proportion of between ties\n",
    "\n",
    "$$p = \\frac{M_{121}}{M_{++1}}$$\n",
    "\n",
    "Now, we calculate the expected value of $p$ Note that the probability of taking a tie between the node of a one group and a node with the other group. Thinking of it as the ratio of Favor cases to total cases, the favor cases are to totall amount of cross ties that are posible. This value will correspond to the number of nodes for group 1 multiplied to the number of nodes from group 2. The number of total cases will correspond to the number of dyads posible in the graph. This will correspond to N choose 2. Conecting this two values we have the following:\n",
    "\n",
    "$$\\pi = \\frac{\\eta_1 \\eta_2}{\\frac{N(N-1)}{2}} = \\frac{2\\eta_1 \\eta_2}{N(N-1)}$$\n",
    "\n",
    "Finally Freeman Segregation Index is defined as:\n",
    "\n",
    "$$S_{Freeman} = 1- \\frac{p}{\\pi}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Daily rutine: 100%|██████████| 63/63 [01:35<00:00,  1.51s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished in 1 minutes with 35.30 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3 Day rutine: 100%|██████████| 61/61 [03:30<00:00,  3.46s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished in 3 minutes with 30.82 seconds\n"
     ]
    }
   ],
   "source": [
    "def process_file(file, categories):\n",
    "    results = []\n",
    "    for pol in categories:\n",
    "        g = gt.load_graph(file)\n",
    "        date = file.split('/')[-1].split('.')[0].split('_')[-1]\n",
    "        seg = Fr.Freeman_Classic(g, types=pol)\n",
    "        results.append(((date, pol), seg))\n",
    "    return results\n",
    "\n",
    "\n",
    "# Run processing in parallel\n",
    "with concurrent.futures.ProcessPoolExecutor() as executor:\n",
    "    # Daily Rutine\n",
    "    tic = perf_counter()\n",
    "    futures = [executor.submit(process_file, file, categories) for file in files_daily]\n",
    "    \n",
    "    # Process results as they complete\n",
    "    for future in tqdm(concurrent.futures.as_completed(futures), total=len(files_daily), desc=\"Daily rutine\"):\n",
    "        try:\n",
    "            results = future.result()\n",
    "            # Update group_segregation DataFrame with results\n",
    "            for key, value in results:\n",
    "                group_segregation_daily.loc[key, 'Classic Freeman'] = value\n",
    "        except Exception as e:\n",
    "            print(f'Generated an exception: {e}')\n",
    "    toc = perf_counter()\n",
    "    time = toc-tic\n",
    "\n",
    "    print(f\"Finished in {time//60:,.0f} minutes with {round(time%60,2):,.2f} seconds\")\n",
    "    # 3 Day Rutine\n",
    "    tic = perf_counter()\n",
    "    futures = [executor.submit(process_file, file, categories) for file in files_3day]\n",
    "    \n",
    "    # Process results as they complete\n",
    "    for future in tqdm(concurrent.futures.as_completed(futures), total=len(files_3day), desc=\"3 Day rutine\"):\n",
    "        try:\n",
    "            results = future.result()\n",
    "            # Update group_segregation DataFrame with results\n",
    "            for key, value in results:\n",
    "                group_segregation_3day.loc[key, 'Classic Freeman'] = value\n",
    "        except Exception as e:\n",
    "            print(f'Generated an exception: {e}')\n",
    "    toc = perf_counter()\n",
    "    time = toc-tic\n",
    "\n",
    "    print(f\"Finished in {time//60:,.0f} minutes with {round(time%60,2):,.2f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Global Freeman Segregation Index (for K groups)\n",
    "\n",
    "For the Freeman Segregation Index, We will use the formula from (Bojanowski & Corten 2014) for the in which the generalize this index for $K$ groups. The index is define as Follows.\n",
    "\n",
    "Let $p$ be equal to the proportion of _between_ group ties in the graph. This corresponds to the upper triangle of the $M$ Matrix without counting the diagonal (This diagonal contains the information of the _within_ group ties).\n",
    "\n",
    "$$p = \\frac{\\sum_{g,h:g\\neq h}M_{gh1}}{\\sum_{g=1}^K\\sum_{h=1}^K M_{gh1}}$$\n",
    "\n",
    "Now, we define the expected proportion of between-group ties in a random graph. In the generalize case of $K$ groups. this looks like this\n",
    "\n",
    "$$\\pi = \\frac{\\left( \\sum_{k=1}^K \\eta_k\\right)^2 - \\sum_{k=1}^K \\eta_k^2}{N(N-1)}$$\n",
    "\n",
    "Finally, Freeman Segregation Index is defined as:\n",
    "\n",
    "$$S_{Freeman} = 1 -\\frac{p}{\\pi} = 1- \\frac{pN(N-1)}{\\left( \\sum_{k=1}^K \\eta_k\\right)^2 - \\sum_{k=1}^K \\eta_k^2}$$\n",
    "\n",
    "This Index takes into account the case **unweighted case**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Daily rutine: 100%|██████████| 63/63 [00:23<00:00,  2.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished in 0 minutes with 23.83 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3 Day rutine: 100%|██████████| 61/61 [00:54<00:00,  1.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished in 0 minutes with 54.43 seconds\n"
     ]
    }
   ],
   "source": [
    "# Storage in DataFrame\n",
    "def process_file(file):\n",
    "    results = []\n",
    "    g = gt.load_graph(file)\n",
    "    date = file.split('/')[-1].split('.')[0].split('_')[-1]\n",
    "    seg = Fr.Freeman_Global(g,property_label = 'Political Label')\n",
    "    results.append((date, seg))\n",
    "    return results\n",
    "\n",
    "# Run processing in parallel\n",
    "with concurrent.futures.ProcessPoolExecutor() as executor:\n",
    "    tic = perf_counter()\n",
    "    # Daily rutine\n",
    "    futures = [executor.submit(process_file, file) for file in files_daily]\n",
    "    \n",
    "    # Process results as they complete\n",
    "    for future in tqdm(concurrent.futures.as_completed(futures), total=len(files_daily), desc=\"Daily rutine\"):\n",
    "        try:\n",
    "            results = future.result()\n",
    "            # Update group_segregation DataFrame with results\n",
    "            for key, value in results:\n",
    "                global_segregation_daily.loc[key, 'Freeman Global'] = value\n",
    "        except Exception as e:\n",
    "            print(f'Generated an exception: {e}')\n",
    "    toc = perf_counter()\n",
    "    time = toc-tic\n",
    "\n",
    "    print(f\"Finished in {time//60:,.0f} minutes with {round(time%60,2):,.2f} seconds\")\n",
    "\n",
    "    tic = perf_counter()\n",
    "    # 3 Day rutine\n",
    "    futures = [executor.submit(process_file, file) for file in files_3day]\n",
    "    \n",
    "    # Process results as they complete\n",
    "    for future in tqdm(concurrent.futures.as_completed(futures), total=len(files_3day), desc=\"3 Day rutine\"):\n",
    "        try:\n",
    "            results = future.result()\n",
    "            # Update group_segregation DataFrame with results\n",
    "            for key, value in results:\n",
    "                global_segregation_3day.loc[key, 'Freeman Global'] = value\n",
    "        except Exception as e:\n",
    "            print(f'Generated an exception: {e}')\n",
    "            \n",
    "    toc = perf_counter()\n",
    "    time = toc-tic\n",
    "\n",
    "    print(f\"Finished in {time//60:,.0f} minutes with {round(time%60,2):,.2f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Freeman Segregation Index for an specific group\n",
    "\n",
    "The Freeman Segregation Index is originally computed for the segregation between two groups. This function will compute the index between one group and all the other ones using _Basic Freeman Segregatioon Index_ Formula. This will give a measure of how segregated is one group over all the others. For this case, our contact layer will only consider two groups, the group $g$ for which one would calculate the index and the group $-g$ wich are all the other nodes that do not belong to $g$. Recall our _Contact Matrix_ that looks like this:\n",
    "\n",
    "$$\n",
    "M_{gh1} = \n",
    "\\begin{bmatrix}\n",
    "    M_{1,1,1} & M_{1,2,1} & \\dots & M_{1,k,1} \\\\\n",
    "    M_{2,1,1} & M_{2,2,1} & \\dots & M_{2,k,1} \\\\\n",
    "    \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "    M_{k,1,1} & M_{k,2,1} & \\dots & M_{k,k,1} \\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "For our calculation, we will  have another _Contact Catrix_ called, \"Me Vs Ohers\" and denoted $\\hat{M}$. This matrix will be a $2\\times 2$. This will be similar as the the original _Contact Matrix_ but with only two groups, $g$ and $-g$. This matrix is defined as follows:\n",
    "\n",
    "$$\n",
    "M^* = \n",
    "\\begin{bmatrix}\n",
    "    M*_{gg} & M*_{g-g} \\\\\n",
    "    M*_{-gg} & M*_{-g-g} \\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $M^*_{g-g} = M_{gg1}$\n",
    "- $M^*_{g-g} = \\sum_{g = 1}^k M_{gh1} - M_{gg1}$\n",
    "- $M^*_{-gg} = \\sum_{h = 1}^k M_{gh1} - M_{gg1}$\n",
    "- $M^*_{-g-g} = \\sum \\sum \\hat{M}_{gh}$\n",
    "\n",
    "For the calculation of the $M*_{-g-g1}$ we substract from the original _Contact Matrix_ the index rows and columns for the group $g$ (Will be denoted as $\\hat{M}$). This will be the contact matrix if this group hadn't existed. Thanks to this matrix, we can compute all the between ties from all nodes that aren't in $g$. This will be the sum of all the values in the matrix. Formally,\n",
    "$$\n",
    "\\hat{M} = \n",
    "    \\begin{bmatrix}\n",
    "    a_{1,1} & \\dots & a_{1,g-1} & a_{1,g+1} & \\dots & a_{1,k} \\\\\n",
    "    a_{2,1} & \\dots & a_{2,g-1} & a_{2,g+1} & \\dots & a_{2,k} \\\\\n",
    "    \\vdots & \\ddots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "    a_{g-1,1} & \\dots & a_{g-1,g-1} & a_{g-1,g+1} & \\dots & a_{g-1,k} \\\\\n",
    "    a_{g+1,1} & \\dots & a_{g+1,g-1} & a_{g+1,g+1} & \\dots & a_{g+1,k} \\\\\n",
    "    \\vdots & \\ddots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "    a_{k,1} & \\dots & a_{k,g-1} & a_{k,g+1} & \\dots & a_{k,k} \\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Now, for the Freeman Formula we compute both $P$ and $\\pi$ and calculate $1-\\frac{P}{\\pi}$\n",
    "\n",
    "$$P = \\frac{M^*_{g-g}}{M^*_{++}}$$\n",
    "\n",
    "$$\\pi = \\frac{2|G_g|*|G_{-g}|}{N(N-1)}$$\n",
    "\n",
    "$$S_{Freeman}^g = 1- \\frac{N(N-1)M^*_{g-g}}{2M^*_{++}|G_g||G_{-g}|}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Daily rutine: 100%|██████████| 63/63 [01:39<00:00,  1.57s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished rutine in 1 minutes with 39.38 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3 Day rutine: 100%|██████████| 61/61 [03:55<00:00,  3.85s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished rutine in 3 minutes with 55.01 seconds\n"
     ]
    }
   ],
   "source": [
    "def process_file(file, categories):\n",
    "    results = []\n",
    "    for pol in categories:\n",
    "        g = gt.load_graph(file)\n",
    "        graph_name = file.split('/')[-1].split('.')[0].split('_')[-1]\n",
    "        seg = Fr.Freeman_Groups(g, 'Political Label', pol)\n",
    "        results.append(((graph_name, pol), seg))\n",
    "    return results\n",
    "\n",
    "# Run processing in parallel\n",
    "with concurrent.futures.ProcessPoolExecutor() as executor:\n",
    "    tic = perf_counter()\n",
    "    # Daily rutine\n",
    "    futures = [executor.submit(process_file, file, categories) for file in files_daily]\n",
    "    \n",
    "    # Process results as they complete\n",
    "    for future in tqdm(concurrent.futures.as_completed(futures), total=len(files_daily), desc=\"Daily rutine\"):\n",
    "        try:\n",
    "            results = future.result()\n",
    "            # Update group_segregation DataFrame with results\n",
    "            for key, value in results:\n",
    "                group_segregation_daily.loc[key, 'Freeman One vs Others'] = value\n",
    "        except Exception as e:\n",
    "            print(f'Generated an exception: {e}')\n",
    "    toc = perf_counter()\n",
    "    time = toc-tic\n",
    "\n",
    "    print(f\"Finished rutine in {time//60:,.0f} minutes with {round(time%60,2):,.2f} seconds\")\n",
    "    \n",
    "    tic = perf_counter()\n",
    "    # 3 Day rutine\n",
    "    futures = [executor.submit(process_file, file, categories) for file in files_3day]\n",
    "    \n",
    "    # Process results as they complete\n",
    "    for future in tqdm(concurrent.futures.as_completed(futures), total=len(files_3day), desc=\"3 Day rutine\"):\n",
    "        try:\n",
    "            results = future.result()\n",
    "            # Update group_segregation DataFrame with results\n",
    "            for key, value in results:\n",
    "                group_segregation_3day.loc[key, 'Freeman One vs Others'] = value\n",
    "        except Exception as e:\n",
    "            print(f'Generated an exception: {e}')\n",
    "    toc = perf_counter()\n",
    "    time = toc-tic\n",
    "\n",
    "    print(f\"Finished rutine in {time//60:,.0f} minutes with {round(time%60,2):,.2f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Give a weighted graph in which the nonnegative weights of each individual's outgoing links sum to $1$,  we can define the proximity of individual $j$ to group  $k$  as: \n",
    "\n",
    "$$Prox_{j\\to k}=\\frac{W_{jk}}{(T_k/ \\sum_{m\\in G} T_m)}$$\n",
    "\n",
    "where $W_{jk}$  is the sum of all the weights that $j$  puts on members of group $k$ and for each group $m$, $A_m$ denotes  the total number of original tweets by members of group $k$ (tweets made on the time period (i.e. day) in question).  $G$ denotes the set of groups in the populations.  So the denominator captures the fraction of $i$'s outgoing mass (which equals 1) that would have gone onto group $k$ if it had been distributed uniformly at random.  i.e.  if agent $i$ had simply distributes its outgoing mass of $1$  uniformly at random  among the $\\sum_{m\\in G} T_m)$ written that day."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Proximidad individual a grupo h: 100%|██████████| 63/63 [21:57<00:00, 20.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished cell in 21 minutes with 57.36 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# HAY QUE MEJORARLO\n",
    "tic = perf_counter()\n",
    "for file in tqdm(files_daily, desc=\"Proximidad individual a grupo h\"):\n",
    "    g = gt.load_graph(file)    \n",
    "    den_dict = {cat: Pr.at_random_scenario(g, 'Political Label', cat, 'Proximity to Group') for cat in categories}\n",
    "\n",
    "    def process_individual_segregation(params,den_dict):\n",
    "        i, cat = params\n",
    "        num = Pr.individual_proximity_to_h(g, i, 'Political Label', cat)\n",
    "        date = g.gp['Date']\n",
    "        den = den_dict[cat]\n",
    "        seg = num/den\n",
    "        return (i, cat), seg, date\n",
    "    def main():\n",
    "        params = [(i, cat) for i in range(len(master_id)) for cat in categories]\n",
    "\n",
    "        # Wrap the function call to include den_dict\n",
    "        individual_segregation_process = partial(process_individual_segregation, den_dict=den_dict)\n",
    "        # Use ProcessPoolExecutor to parallelize the computation\n",
    "        with concurrent.futures.ProcessPoolExecutor() as executor:\n",
    "            # Map the function over the parameters and wrap with tqdm for progress bar\n",
    "            results = executor.map(individual_segregation_process, params)\n",
    "\n",
    "        # Populate the DataFrame with results\n",
    "        for row_index, result, date in results:\n",
    "            individual_group_segregation.loc[row_index, f'Proximity index on {date}'] = result\n",
    "\n",
    "    if __name__ == '__main__':\n",
    "        main()\n",
    "toc = perf_counter()\n",
    "time = toc-tic\n",
    "\n",
    "print(f\"Finished cell in {time//60:,.0f} minutes with {round(time%60,2):,.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tic = perf_counter()\n",
    "for file in tqdm(files_daily, desc = \"Proximidad a Otros\"):\n",
    "    g = gt.load_graph(file)    \n",
    "    den_dict = {cat: Pr.at_random_scenario(g, 'Political Label', cat, 'Proximity to Others') for cat in categories}\n",
    "\n",
    "    def process_individual_segregation(i, den_dict):\n",
    "        num = Pr.individual_proximity_to_others(g, i, 'Political Label')\n",
    "        date = g.gp['Date']\n",
    "        den = den_dict[g.vp['Political Label'][g.vertex(i)]]\n",
    "        seg = num/den\n",
    "        return i, seg, date\n",
    "    def main():\n",
    "        # Wrap the function call to include den_dict\n",
    "        individual_segregation_process = partial(process_individual_segregation, den_dict=den_dict)\n",
    "        # Use ProcessPoolExecutor to parallelize the computation\n",
    "        with concurrent.futures.ProcessPoolExecutor() as executor:\n",
    "            # Map the function over the parameters and wrap with tqdm for progress bar\n",
    "            results = executor.map(individual_segregation_process, range(len(master_id)))\n",
    "\n",
    "        # Populate the DataFrame with results\n",
    "        for row_index, result, date in results:\n",
    "            individual_node_segregation.loc[row_index, f'Proximity to Others on {date}'] = result\n",
    "\n",
    "    if __name__ == '__main__':\n",
    "        main()\n",
    "toc = perf_counter()\n",
    "time = toc-tic\n",
    "\n",
    "print(f\"Finished cell in {time//60:,.0f} minutes with {round(time%60,2):,.2f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 3. Proximity Index\n",
    "\n",
    "### Index of attention from that group $g$ devotes to others ($-g$)\n",
    "\n",
    "Using the same philosophy from Freeman index. Here we will calculte the coeficient between the proportion of cross ties in the graph, against the random scenario. For this case, we will take in account wwights and directionality of the graph. For that matter, take in account a _Contact Layer_ in which the entry $M_{g,-g}$ corresponds to the summ of all the weights the nodes from group $g$ devotes to any other group $-g$ In that cases, we define $P$ as:\n",
    "\n",
    "$$P = \\frac{M^*_{g-g}}{M^*_{++}}$$\n",
    "\n",
    "Recalling the construction of the weights, the sum of all the weights the comes out of a nodes sums up to one (The sum of every row in the weighted adjacency matrix corresponds to one). \n",
    "\n",
    "For the expected value of $P$ which we called $\\pi$ the number of cross ties weights will be calculated as the amount of weights the gruop $g$ would randomly devote to other $-g$. An edge is made between two nodes $i$ and $j$ if $i$ retweeted $j$. So the expected weight of $i$ could devote to another person $j$ will depend of the amount of original tweets that $j$ made and can be retweeted by $i$. With out loss of generality, we can say that the expected total weights from $g$ to $-g$ will correspond to the total amount of original tweets made from $-g$ nodes. over the total amount of tweets made that day.\n",
    "\n",
    "We define then T$_i$ as the amount of original tweets made by $i$ and also we define the amount of tweets made by the group $g$ as\n",
    "$$T^g = \\sum_{i\\in G_g} T_i$$\n",
    "\n",
    "Consecuently, the amount of tweets made by other groups other than $g$ will be\n",
    "$$T^{-g} = \\sum_{i\\notin G_g} T_i$$\n",
    "\n",
    "$$\\pi = \\frac{T^-g}{T^+}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_file(file, categories):\n",
    "    results = []\n",
    "    for pol in categories:\n",
    "        g = gt.load_graph(file)\n",
    "        date = file.split('/')[-1].split('.')[0].split('_')[-1]\n",
    "        num = Pr.proximity_g_others(g, 'Political Label', 'Normal Weight', pol)\n",
    "        den = Pr.at_random_scenario(g, 'Political Label', pol, 'Proximity to Others')\n",
    "        seg = num/den\n",
    "        results.append(((date, pol), seg))\n",
    "    return results\n",
    "\n",
    "# Run processing in parallel\n",
    "with concurrent.futures.ProcessPoolExecutor() as executor:\n",
    "    # Daily rutine\n",
    "    futures = [executor.submit(process_file, file, categories) for file in files_daily]\n",
    "    \n",
    "    # Process results as they complete\n",
    "    for future in tqdm(concurrent.futures.as_completed(futures), total=len(files_daily)):\n",
    "        try:\n",
    "            results = future.result()\n",
    "            # Update group_segregation DataFrame with results\n",
    "            for key, value in results:\n",
    "                group_segregation_daily.loc[key, 'Proximity to Others'] = value\n",
    "        except Exception as e:\n",
    "            print(f'Generated an exception: {e}')\n",
    "    \n",
    "    # 3 Day rutine\n",
    "    futures = [executor.submit(process_file, file, categories) for file in files_3day]\n",
    "    \n",
    "    # Process results as they complete\n",
    "    for future in tqdm(concurrent.futures.as_completed(futures), total=len(files_3day)):\n",
    "        try:\n",
    "            results = future.result()\n",
    "            # Update group_segregation DataFrame with results\n",
    "            for key, value in results:\n",
    "                group_segregation_3day.loc[key, 'Proximity to Others'] = value\n",
    "        except Exception as e:\n",
    "            print(f'Generated an exception: {e}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_file(file, categories):\n",
    "    results = []\n",
    "    for pol in categories:\n",
    "        g = gt.load_graph(file)\n",
    "        graph_name = file.split('/')[-1].split('.')[0].split('_')[-1]\n",
    "        num = Pr.proximity_g_others(g, 'Political Label', 'Normal Weight', pol, in_proximity=False)\n",
    "        den = Pr.at_random_scenario(g, 'Political Label', pol, 'Proximity to Others')\n",
    "        seg = num/den\n",
    "        results.append(((graph_name, pol), seg))\n",
    "    return results\n",
    "\n",
    "# Run processing in parallel\n",
    "with concurrent.futures.ProcessPoolExecutor() as executor:\n",
    "    futures = [executor.submit(process_file, file, categories) for file in files_daily]\n",
    "    \n",
    "    # Process results as they complete\n",
    "    for future in tqdm(concurrent.futures.as_completed(futures), total=len(files_daily)):\n",
    "        try:\n",
    "            results = future.result()\n",
    "            # Update group_segregation DataFrame with results\n",
    "            for key, value in results:\n",
    "                group_segregation_daily.loc[key, \"Other's Proximity\"] = value\n",
    "        except Exception as e:\n",
    "            print(f'Generated an exception: {e}')\n",
    "    \n",
    "    # 3 Day rutine\n",
    "    futures = [executor.submit(process_file, file, categories) for file in files_3day]\n",
    "    \n",
    "    # Process results as they complete\n",
    "    for future in tqdm(concurrent.futures.as_completed(futures), total=len(files_3day)):\n",
    "        try:\n",
    "            results = future.result()\n",
    "            # Update group_segregation DataFrame with results\n",
    "            for key, value in results:\n",
    "                group_segregation_3day.loc[key, \"Other's Proximity\"] = value\n",
    "        except Exception as e:\n",
    "            print(f'Generated an exception: {e}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Index of attention from that group $h$ devotes an specific group ($k$)\n",
    "\n",
    "$$Prox_{h\\to k}=\\frac{(W_{hk}/A_h)}{(T_k/ \\sum_{m\\in G} T_m)}$$\n",
    "\n",
    "where  $W_{hk}$  is the sum of all the weights that members of group $h$  put on members of group $k$  and $A_h$  is the number  of  retweet-active members of group $h$ (on the day in  question). That is, the number of members of group $h$ who rewteeted at least one original tweet of some member of the whole community on that day, or what is equivalent: the number of members of group $h$ whose rows in the adjacency matrix (of that day) sum to 1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_file(file, categories):\n",
    "    results = []\n",
    "    for pol_in in categories:\n",
    "        for pol_out in categories:\n",
    "            g = gt.load_graph(file)\n",
    "            date = file.split('/')[-1].split('.')[0].split('_')[-1]\n",
    "            num = Pr.proximity_g_h(g, 'Political Label', 'Normal Weight', pol_in, pol_out)\n",
    "            den = Pr.at_random_scenario(g, 'Political Label', pol_out, 'Proximity to Group')\n",
    "            seg = num/den\n",
    "            results.append(((date, pol_out), pol_in, seg))\n",
    "    return results\n",
    "\n",
    "# Run processing in parallel\n",
    "with concurrent.futures.ProcessPoolExecutor() as executor:\n",
    "    # Daily rutine\n",
    "    futures = [executor.submit(process_file, file, categories) for file in files_daily]\n",
    "    \n",
    "    # Process results as they complete\n",
    "    for future in tqdm(concurrent.futures.as_completed(futures), total=len(files_daily)):\n",
    "        try:\n",
    "            results = future.result()\n",
    "            # Update group_segregation DataFrame with results\n",
    "            for key, var, value in results:\n",
    "                group_segregation_daily.loc[key, f'Proximity From {var} To'] = value\n",
    "        except Exception as e:\n",
    "            print(f'Generated an exception: {e}')\n",
    "    \n",
    "    futures = [executor.submit(process_file, file, categories) for file in files_3day]\n",
    "    \n",
    "    # 3 Day rutine\n",
    "    # Process results as they complete\n",
    "    for future in tqdm(concurrent.futures.as_completed(futures), total=len(files_3day)):\n",
    "        try:\n",
    "            results = future.result()\n",
    "            # Update group_segregation DataFrame with results\n",
    "            for key, var, value in results:\n",
    "                group_segregation_3day.loc[key, f'Proximity From {var} To'] = value\n",
    "        except Exception as e:\n",
    "            print(f'Generated an exception: {e}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_file(file, categories):\n",
    "    results = []\n",
    "    for pol1 in categories:\n",
    "        for pol2 in categories:\n",
    "            g = gt.load_graph(file)\n",
    "            date = file.split('/')[-1].split('.')[0].split('_')[-1]\n",
    "            num = Pr.proximity_g_h(g, 'Political Label', 'Normal Weight', pol1, pol2, in_proximity=False)\n",
    "            den = Pr.at_random_scenario(g, 'Political Label', pol2, 'Proximity to Group')\n",
    "            seg =num/den\n",
    "            results.append(((date, pol2), pol1, seg))\n",
    "    return results\n",
    "\n",
    "# Run processing in parallel\n",
    "with concurrent.futures.ProcessPoolExecutor() as executor:\n",
    "    # Daily rutine\n",
    "    futures = [executor.submit(process_file, file, categories) for file in files_daily]\n",
    "    \n",
    "    # Process results as they complete\n",
    "    for future in tqdm(concurrent.futures.as_completed(futures), total=len(files_daily)):\n",
    "        try:\n",
    "            results = future.result()\n",
    "            # Update group_segregation DataFrame with results\n",
    "            for key, var, value in results:\n",
    "                group_segregation_daily.loc[key, f'Proximity {var} Took From'] = value\n",
    "        except Exception as e:\n",
    "            print(f'Generated an exception: {e}')\n",
    "    \n",
    "    # 3 Day rutine\n",
    "    futures = [executor.submit(process_file, file, categories) for file in files_3day]\n",
    "    \n",
    "    # Process results as they complete\n",
    "    for future in tqdm(concurrent.futures.as_completed(futures), total=len(files_3day)):\n",
    "        try:\n",
    "            results = future.result()\n",
    "            # Update group_segregation DataFrame with results\n",
    "            for key, var, value in results:\n",
    "                group_segregation_3day.loc[key, f'Proximity {var} Took From'] = value\n",
    "        except Exception as e:\n",
    "            print(f'Generated an exception: {e}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 4. Assorativity\n",
    "\n",
    "- **Assortativity:** is a preference for a network's nodes to attach to others that are similar in some way. Though the specific measure of similarity may vary, network theorists often examine assortativity in terms of a node's degree.\n",
    "\n",
    "    The **assortativity coefficient** is the Pearson correlation coefficient of degree between pairs of linked nodes. Positive values of `r` indicate a correlation between nodes of similar degree, while negative values indicate relationships between nodes of different degree. In general, `r` lies between `−1` and `1`. When `r = 1`, the network is said to have perfect assortative mixing patterns, when `r = 0` the network is non-assortative, while at `r = −1` the network is completely disassortative.\n",
    "\n",
    "    The *assortativity coefficient* is given by \n",
    "\n",
    "    $$\n",
    "    r = \\frac{\\sum_{jk}{jk (e_{jk} - q_j q_k)}}{\\sigma_{q}^{2}}\n",
    "    $$\n",
    "\n",
    "    In this equation:\n",
    "\n",
    "    - $ \\sum_{jk} $ denotes the summation over all degrees $ j $ and $ k $ in the network.\n",
    "    - $ jk $ represents the product of degrees $ j $ and $ k $.\n",
    "    - $ e_{jk} $ is the joint probability distribution of the remaining degrees of two connected vertices. In an undirected graph, this is symmetric and must satisfy the sum rules:\n",
    "        - $ \\sum_{jk}{e_{jk}} = 1 $, ensuring that the total probability is 1.\n",
    "        - $ \\sum_{j}{e_{jk}} = q_{k} $, linking it to the distribution of the remaining degree.\n",
    "    - $ q_j $ and $ q_k $ are the distributions of the remaining degree for vertices of degrees $ j $ and $ k $, respectively. \n",
    "    - $ \\sigma_{q}^{2} $ is the variance of the distribution of the remaining degree.\n",
    "\n",
    "    The term $ q_{k} $ represents the distribution of the *remaining degree*, which captures the number of edges leaving a node, excluding the edge that connects the pair in question. This distribution is derived from the degree distribution $ p_{k} $ as follows:\n",
    "\n",
    "    $$\n",
    "    q_{k} = \\frac{(k+1)p_{k+1}}{\\sum_{j \\geq 1} j p_j}\n",
    "    $$\n",
    "\n",
    "    - Here, $ p_{k} $ is the degree distribution of the network, and $ p_{k+1} $ refers to the probability of a node having $ k+1 $ connections.\n",
    "\n",
    "\n",
    "- **Categorical Assortativity (assortativity by attribute):** is a measure used to determine how often nodes with a certain categorical attribute, like color or type, connect to other nodes with the same attribute. It is given by:\n",
    "\n",
    "    $$\n",
    "    r = \\frac{\\sum_{ij}{e_{ij} - q_i q_j}}{\\sum_{i}{q_i q_i} - \\sum_{i}{q_i q_j}}\n",
    "    $$\n",
    "\n",
    "    Where:\n",
    "\n",
    "    - $ e_{ij} $ is the proportion of edges in the network that connect nodes of type $ i $ to nodes of type $ j $.\n",
    "    - $ q_i $ and $ q_j $ are the proportions of each type of node (type $ i $ and $ j $, respectively) at the ends of a randomly chosen edge.\n",
    "\n",
    "    In this context:\n",
    "\n",
    "    - A positive value of $ r $ indicates assortative mixing, where nodes tend to connect to others that are similar.\n",
    "    - A negative value of $ r $ indicates disassortative mixing, where nodes tend to connect to others that are different.\n",
    "    - A value of $ r $ close to 0 suggests no particular preference for nodes to connect to others based on the categorical attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Storage in DataFrame\n",
    "for file in tqdm(files_daily):\n",
    "    g = gt.load_graph(file)\n",
    "    date = file.split('/')[-1].split('.')[0].split('_')[-1]\n",
    "    for pol in categories:\n",
    "        # Non weighted\n",
    "        seg_w = gt.assortativity(g, g.vp[pol])\n",
    "        group_segregation_daily.loc[(date, pol), 'Non Weighted Assortativity'] = seg_w[0]\n",
    "        \n",
    "        # Weighted\n",
    "        seg_no_w = gt.assortativity(g, g.vp[pol], eweight=g.ep['Normal Weight'])\n",
    "        group_segregation_daily.loc[(date, pol), 'Normal Weighted Assortativity'] = seg_no_w[0]\n",
    "        \n",
    "        seg_no_w = gt.assortativity(g, g.vp[pol], eweight=g.ep['Number of rts'])\n",
    "        group_segregation_daily.loc[(date, pol), 'Weighted Assortativity'] = seg_no_w[0]\n",
    "        \n",
    "    #Global\n",
    "    seg = gt.assortativity(g, g.vp['Political Label'], eweight=g.ep['Normal Weight'])\n",
    "    global_segregation_daily.loc[(date), 'Normal Weighted Assortativity'] = seg[0]\n",
    "    \n",
    "    seg = gt.assortativity(g, g.vp['Political Label'], eweight=g.ep['Number of rts'])\n",
    "    global_segregation_daily.loc[(date), 'Weighted Assortativity'] = seg[0]\n",
    "    \n",
    "    seg = gt.assortativity(g, g.vp['Political Label'])\n",
    "    global_segregation_daily.loc[(date), 'Non Weighted Assortativity'] = seg[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Storage in DataFrame\n",
    "for file in tqdm(files_3day):\n",
    "    g = gt.load_graph(file)\n",
    "    date = file.split('/')[-1].split('.')[0].split('_')[-1]\n",
    "    for pol in categories:\n",
    "        # Non weighted\n",
    "        seg_w = gt.assortativity(g, g.vp[pol])\n",
    "        group_segregation_3day.loc[(date, pol), 'Non Weighted Assortativity'] = seg_w[0]\n",
    "        \n",
    "        # Weighted\n",
    "        seg_no_w = gt.assortativity(g, g.vp[pol], eweight=g.ep['Normal Weight'])\n",
    "        group_segregation_3day.loc[(date, pol), 'Normal Weighted Assortativity'] = seg_no_w[0]\n",
    "        \n",
    "        seg_no_w = gt.assortativity(g, g.vp[pol], eweight=g.ep['Number of rts'])\n",
    "        group_segregation_3day.loc[(date, pol), 'Weighted Assortativity'] = seg_no_w[0]\n",
    "        \n",
    "    #Global\n",
    "    seg = gt.assortativity(g, g.vp['Political Label'], eweight=g.ep['Normal Weight'])\n",
    "    global_segregation_3day.loc[(date), 'Normal Weighted Assortativity'] = seg[0]\n",
    "    \n",
    "    seg = gt.assortativity(g, g.vp['Political Label'], eweight=g.ep['Number of rts'])\n",
    "    global_segregation_3day.loc[(date), 'Weighted Assortativity'] = seg[0]\n",
    "    \n",
    "    seg = gt.assortativity(g, g.vp['Political Label'])\n",
    "    global_segregation_3day.loc[(date), 'Non Weighted Assortativity'] = seg[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 5. Homiphily Index\n",
    "\n",
    "refers to the tendency of individuals (or nodes in a network) to associate and bond with similar others. The similarity can be based on various attributes such as social characteristics, behaviors, or beliefs. In the context of a network, this implies that nodes are more likely to form connections with other nodes that belong to the same group or share similar attributes. \n",
    "\n",
    "Measuring Homophily. We begin with some simple definitions that are important in measuring homophily and also in presenting the model.\n",
    "\n",
    "Let $ N $ denote the number of type $ i $ individuals in the population, and let $ w_i = \\frac{N_i}{N} $ be the relative fraction of type $ i $ in the population, where $ N = \\sum_k N_k $.\n",
    "\n",
    "Let $ s_i $ denote the average number of friendships that agents of type $ i $ have with agents who are of the same type, and let $ d_i $ be the average number of friendships that type $ i $ agents form with agents of types different from $ i $. Let $ t_i = s_i + d_i $ be the average total number of friendships that type $ i $ agents form.\n",
    "\n",
    "The homophily index $ H_i $ measures the fraction of the ties of individuals of type $ i $ that are with that same type.\n",
    "\n",
    "**Definition 1** The homophily index $ H_i $ is defined by\n",
    "\n",
    "$$ H_i = \\frac{s_i}{s_i + d_i} $$\n",
    "\n",
    "The profile $ (s, d) $ exhibits *baseline homophily* for type $ i $ if $ H_i = w_i $.\n",
    "\n",
    "The profile $ (s, d) $ exhibits *inbreeding homophily* for type $ i $ if $ H_i > w_i $.\n",
    "\n",
    "Generally, there is a difficulty in simply measuring homophily according to $ H_i $. For example, consider a group that comprises 95% of a population. Suppose that its same-type friendships are 95% of its friendships. Compare this to a group that comprises 5% of a population and has 96% of its friendships being same-type. Although both have the same homophily index, they are very different in terms of how homophilous they are relative to how homophilous they could be. Comparing the homophily index, $ H_i $, to the baseline, $ w_i $, provides some information, but even that does not fully capture the idea of how biased a group is compared to how biased it could potentially be. To take care of this we use the inbreeding homophily index introduced by Coleman [Coleman J. (1958) *Human Organization* 17:28–36] that normalizes the homophily index by the potential extent to which a group could be biased.\n",
    "\n",
    "**Definition 2** Coleman's inbreeding homophily index of type $i$ is\n",
    "\n",
    "$$IH_i = \\frac{H_i - w_i}{1 - w_i}$$\n",
    "\n",
    "This index measures the amount of bias with respect to baseline homophily as it relates to the maximum possible bias (the term $ 1 - w_i $). It can be easily checked that we have inbreeding homophily for type $ i $ if and only if $ IH_i > 0 $, and inbreeding heterophily for type $ i $ if and only if $ IH_i < 0 $. The index of inbreeding homophily is 0 if there is pure baseline homophily, and 1 if a group completely inbreeds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_file(file, categories):\n",
    "    results = []\n",
    "    g = gt.load_graph(file)\n",
    "    date = file.split('/')[-1].split('.')[0].split('_')[-1]\n",
    "    Homiphily_dict = Ho.homophily_index(graph = g, property_name = \"Political Label\")\n",
    "    H = Homiphily_dict ['H_i']\n",
    "    IH = Homiphily_dict ['IH_i']\n",
    "    for pol in categories:\n",
    "        results.append(((date, pol), H[pol], IH[pol]))\n",
    "    return results\n",
    "\n",
    "# Run processing in parallel\n",
    "with concurrent.futures.ProcessPoolExecutor() as executor:\n",
    "    # Daily rutine\n",
    "    futures = [executor.submit(process_file, file, categories) for file in files_daily]\n",
    "    \n",
    "    # Process results as they complete\n",
    "    for future in tqdm(concurrent.futures.as_completed(futures), total=len(files_daily)):\n",
    "        try:\n",
    "            results = future.result()\n",
    "            # Update group_segregation DataFrame with results\n",
    "            for key, H_value, IH_value in results:\n",
    "                group_segregation_daily.loc[key, 'Homiphily Index'] = H_value\n",
    "                group_segregation_daily.loc[key, 'Inbreeding Homiphily Index'] = IH_value\n",
    "        except Exception as e:\n",
    "            print(f'Generated an exception: {e}')\n",
    "    \n",
    "    # 3 Day rutine\n",
    "    futures = [executor.submit(process_file, file, categories) for file in files_3day]\n",
    "    \n",
    "    # Process results as they complete\n",
    "    for future in tqdm(concurrent.futures.as_completed(futures), total=len(files_3day)):\n",
    "        try:\n",
    "            results = future.result()\n",
    "            # Update group_segregation DataFrame with results\n",
    "            for key, H_value, IH_value in results:\n",
    "                group_segregation_3day.loc[key, 'Homiphily Index'] = H_value\n",
    "                group_segregation_3day.loc[key, 'Inbreeding Homiphily Index'] = IH_value\n",
    "        except Exception as e:\n",
    "            print(f'Generated an exception: {e}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 6. Spectral Segregation Index\n",
    "\n",
    "Explicación pendiente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CODIGOOOOOOOOOOOOOOOOOO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "group_segregation_daily.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_segregation_daily.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "individual_group_segregation.sort_index(axis=1, inplace=True)\n",
    "individual_group_segregation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "individual_node_segregation.reset_index(names='ID', inplace=True)\n",
    "individual_node_segregation.sort_index(axis=1, inplace=True)\n",
    "individual_node_segregation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CHECKPOINT: Save DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run to save\n",
    "group_segregation_daily.to_pickle(os.path.join(path,\"Segregation\",'group_segregation_daily.pkl'))\n",
    "global_segregation_daily.to_pickle(os.path.join(path,\"Segregation\",'global_segregation_daily.pkl'))\n",
    "\n",
    "group_segregation_3day.to_pickle(os.path.join(path,\"Segregation\",'group_segregation_3day.pkl'))\n",
    "global_segregation_3day.to_pickle(os.path.join(path,\"Segregation\",'global_segregation_3day.pkl'))\n",
    "\n",
    "individual_group_segregation.to_pickle(os.path.join(path,\"Segregation\",'individual_group_segregation.pkl'))\n",
    "individual_node_segregation.to_pickle(os.path.join(path,\"Segregation\",'individual_node_segregation.pkl'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CHECKPOINT: Proximity Vectorizado"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Index of Proximity Between Groups\n",
    "\n",
    "$$Prox_{j\\rightarrow k}=\\frac{W_{jk}}{(T_k/\\sum_{m\\in G} T_m)}$$\n",
    "\n",
    "### Measure $W_{jk}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load graphs\n",
    "files = glob(os.path.join(path_daily,\"Graphs\", \"*.graphml\"))\n",
    "files = np.sort(files)\n",
    "\n",
    "results = []\n",
    "\n",
    "for file in tqdm(files):    \n",
    "    # Importamos el grafo\n",
    "    g = gt.load_graph(file)\n",
    "    graph_date = re.search(\"(\\d{4}-\\d{2}-\\d{2})\", file).group(1)\n",
    "\n",
    "    # Número de vertices/individuos\n",
    "    n_individuos = g.num_vertices()\n",
    "    \n",
    "    # Identifica las afiliaciones políticas únicas y asigna índices\n",
    "    political_labeling = np.array([g.vp[\"Political Label\"][j] for j in range(n_individuos)]) \n",
    "    unique_affiliations = np.unique(political_labeling)\n",
    "    affiliation_to_index = {affiliation: i for i, affiliation in enumerate(unique_affiliations)}\n",
    "            \n",
    "    # Contamos el número de rts de cada individuo hacia cada afiliación política\n",
    "    # +2 es porque necesitamos una columna de mismo y otros \n",
    "    results_matrix = np.zeros((n_individuos, len(unique_affiliations) + 2))\n",
    "    for e in g.edges():\n",
    "        s = int(e.source())\n",
    "        t = int(e.target())\n",
    "        rts = g.ep['Number of rts'][e]\n",
    "        affiliation_index = affiliation_to_index[political_labeling[t]]\n",
    "        results_matrix[s, affiliation_index] += rts\n",
    "        # Si es un rt a alguien de la misma afiliación política\n",
    "        if political_labeling[s] == political_labeling[t]:\n",
    "            results_matrix[s, len(unique_affiliations)] += rts\n",
    "        # Si es un rt a alguien de diferente afiliación política\n",
    "        else:\n",
    "            results_matrix[s, len(unique_affiliations) + 1] += rts\n",
    "    \n",
    "    # Calcular la matriz normalizada como un porcentaje del total de RTs salientes por nodo\n",
    "    total_rts_por_nodo = results_matrix[:, 0:len(unique_affiliations)].sum(axis = 1, keepdims = True)\n",
    "    total_rts_por_nodo2 = total_rts_por_nodo[:, [0]*results_matrix.shape[1]]\n",
    "    # Calculamos W_jk\n",
    "    with np.errstate(divide = 'ignore', invalid = 'ignore'):\n",
    "        results_matrix_normalizada = np.divide(results_matrix, total_rts_por_nodo2)\n",
    "    # Luego, reemplaza los valores donde total_rts_por_nodo2 es 0 con NaN\n",
    "    # Esto incluye manejar divisiones 0/0 y valores/0\n",
    "    results_matrix_normalizada[total_rts_por_nodo2 == 0] = np.nan\n",
    "\n",
    "    # Construir diccionario para consolidar resultados\n",
    "    temp = {\n",
    "        \"Nodo_ID\": list(range(n_individuos)),\n",
    "        \"Political_Affiliation\": political_labeling,\n",
    "        \"Date\": graph_date,\n",
    "        \"Total_RTs\": total_rts_por_nodo.flatten()\n",
    "    }\n",
    "\n",
    "    additional_categories = np.array([\"Mismo\", \"Otros\"])\n",
    "\n",
    "    # Concatena unique_affiliations con additional_categories\n",
    "    extended_affiliations = np.concatenate((unique_affiliations, additional_categories))\n",
    "\n",
    "    # Añade las columnas de RTs por afiliación política\n",
    "    for i, affiliation in enumerate(extended_affiliations):\n",
    "        temp[f\"rts_j_{affiliation}\"] = results_matrix[:, i]\n",
    "        temp[f\"W_j_{affiliation}\"] = results_matrix_normalizada[:, i]\n",
    "\n",
    "    df_temp = pd.DataFrame(temp)\n",
    "    results.append(df_temp)\n",
    "\n",
    "W_jk = pd.concat(results, ignore_index = True)\n",
    "W_jk.to_pickle(path = os.path.join(path,\"Segregation\",\"W_jk.gzip\"), compression = \"gzip\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Denominador"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "\n",
    "for file in tqdm(files):  \n",
    "    # Importamos el grafo\n",
    "    g = gt.load_graph(file)\n",
    "    graph_date = re.search(\"(\\d{4}-\\d{2}-\\d{2})\", file).group(1)\n",
    "\n",
    "    # Número de vertices/individuos\n",
    "    n_individuos = g.num_vertices()\n",
    "\n",
    "    # Vamos a calcular el número de tweets por día para cada afiliación política\n",
    "\n",
    "    # Identifica las afiliaciones políticas únicas y asigna índices\n",
    "    political_labeling = np.array([g.vp[\"Political Label\"][j] for j in range(n_individuos)]) \n",
    "    unique_affiliations = np.unique(political_labeling)\n",
    "    affiliation_to_index = {affiliation: i for i, affiliation in enumerate(unique_affiliations)}\n",
    "                \n",
    "    # Contamos el número de tweets de cada individuo según su afiliación política\n",
    "    # +2 es porque necesitamos una columna de mismo y otros \n",
    "    results_matrix = np.zeros(len(unique_affiliations))\n",
    "    for v in g.vertices():\n",
    "        n = g.vp[\"Tweets\"][v]\n",
    "        pl = g.vp[\"Political Label\"][v]\n",
    "        affiliation_index = affiliation_to_index[pl]\n",
    "        results_matrix[affiliation_index] += n\n",
    "\n",
    "    # Ahora calculamos el denominador para cada afiliación\n",
    "    total = results_matrix.sum()\n",
    "    denominadores = results_matrix/total\n",
    "\n",
    "\n",
    "    # Ahora vamos a construir el denominador para cada individuo\n",
    "    denominador = np.zeros((n_individuos, 2))\n",
    "    for v in g.vertices():\n",
    "        pl = g.vp[\"Political Label\"][v]\n",
    "        affiliation_index = affiliation_to_index[pl]\n",
    "        mismo = results_matrix[affiliation_index]/total\n",
    "        otros = 1 - mismo\n",
    "        denominador[int(v), :] = [mismo, otros]\n",
    "\n",
    "    # Construir diccionario para consolidar resultados\n",
    "    temp = {\n",
    "        \"Nodo_ID\": list(range(n_individuos)),\n",
    "        \"Political_Affiliation\": political_labeling,\n",
    "        \"Date\": graph_date,\n",
    "        \"Denominador Centro\": denominadores[0],\n",
    "        \"Denominador Derecha\": denominadores[1],\n",
    "        \"Denominador Izquierda\": denominadores[2],\n",
    "        \"Denominador Sin Clasificar\": denominadores[3],\n",
    "        \"Denominador Mismo\": denominador[:, 0].flatten(),\n",
    "        \"Denominador Otros\": denominador[:, 1].flatten()\n",
    "    }\n",
    "\n",
    "    df_temp = pd.DataFrame(temp)\n",
    "    results.append(df_temp)\n",
    "denominador = pd.concat(results, ignore_index = True)\n",
    "\n",
    "denominador.to_pickle(path = os.path.join(path,\"Segregation\",\"denominador.gzip\"), compression = \"gzip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparativos para proximidad\n",
    "\n",
    "num = W_jk[[\"W_j_Centro\", \"W_j_Derecha\", \"W_j_Izquierda\", \"W_j_Sin Clasificar\", \"W_j_Mismo\", \"W_j_Otros\"]].values\n",
    "dem = denominador.iloc[:, 3::].values\n",
    "proximidad = pd.DataFrame(num/dem, columns = [\"P_Centro\", \"P_Derecha\", \"P_Izquierda\", \"P_Sin Clasificar\", \"P_Mismo\", \"P_Otros\"])\n",
    "proximidad = pd.concat([W_jk.iloc[:, :4], proximidad], axis = 1)\n",
    "proximidad.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "proximidad.to_pickle(path = os.path.join(path,\"Segregation\",\"proximidad.gzip\"), compression = \"gzip\")\n",
    "proximidad.to_csv(os.path.join(path,\"Segregation\",\"proximidad.csv\"),index=False, sep=';')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prueba de Cálculos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carga de DataFrames\n",
    "\n",
    "W_jk = pd.read_pickle(os.path.join(path,\"Segregation\",\"W_jk.gzip\"), compression = \"gzip\")\n",
    "denominador = pd.read_pickle(os.path.join(path,\"Segregation\",\"denominador.gzip\"), compression = \"gzip\")\n",
    "proximidad = pd.read_pickle(os.path.join(path,\"Segregation\",\"proximidad.gzip\"), compression = \"gzip\")\n",
    "\n",
    "# Valores de prueba\n",
    "grupo = 'Centro'\n",
    "vertice = 3\n",
    "fecha = '2021-05-04'\n",
    "\n",
    "# Grafo de prueba\n",
    "prueba = f\"starting_{fecha}.graphml\"\n",
    "os.path.join(path_daily,\"Graphs\",prueba)\n",
    "G = gt.load_graph(os.path.join(path_daily,\"Graphs\",prueba))\n",
    "\n",
    "w_jk_grupo = Pr.individual_proximity_to_h(G,vertice,'Political Label',grupo)\n",
    "w_jk_otros = Pr.individual_proximity_to_others(G,vertice,'Political Label')\n",
    "den = Pr.at_random_scenario(G,'Political Label', grupo, 'Proximity to Group')\n",
    "\n",
    "# Calculos Fernando\n",
    "print('Calculos Fernando')\n",
    "print()\n",
    "print(f\"Proximidad del Nodo {vertice} en fecha {fecha} a grupo {grupo}:\")\n",
    "print(f'Numerador:  {w_jk_grupo}')\n",
    "print(f'Denominador:  {den}')\n",
    "print(f\"proximidad a {grupo}: {w_jk_grupo/den}\")\n",
    "print()\n",
    "print(f\"Proximidad del Nodo {vertice} en fecha {fecha} a otros grupos\")\n",
    "print(f'Numerador:  {w_jk_otros}')\n",
    "print(f'Denominador:  {den}')\n",
    "print(f\"proximidad a Otros: {w_jk_otros/den}\")\n",
    "\n",
    "print(\"\\n\"+\"-\"*100+\"\\n\")\n",
    "\n",
    "# Calculos Lucas\n",
    "proximidad_grupo = proximidad[f\"P_{grupo}\"][(proximidad[\"Nodo_ID\"] == vertice) & (proximidad[\"Date\"] == fecha)].iloc[0]\n",
    "W_jk_grupo = W_jk[f\"W_j_{grupo}\"][(W_jk[\"Nodo_ID\"] == vertice) & (W_jk[\"Date\"] == fecha)].iloc[0]\n",
    "den = denominador[f\"Denominador {grupo}\"][(denominador[\"Nodo_ID\"] == vertice) & (denominador[\"Date\"] == fecha)].iloc[0]\n",
    "print('Calculos Lucas')\n",
    "print()\n",
    "print(f\"Proximidad del Nodo {vertice} en fecha {fecha} a grupo {grupo} En DataFrame\")\n",
    "print(f\"Numerador: {W_jk_grupo}\")\n",
    "print(f\"Denominador: {den}\")\n",
    "print(f\"Proximidad a {grupo}: {proximidad_grupo}\")\n",
    "print()\n",
    "\n",
    "proximidad_otros = proximidad[f\"P_Otros\"][(proximidad[\"Nodo_ID\"] == vertice) & (proximidad[\"Date\"] == fecha)].iloc[0]\n",
    "W_jk_otros = W_jk[f\"W_j_Otros\"][(W_jk[\"Nodo_ID\"] == vertice) & (W_jk[\"Date\"] == fecha)].iloc[0]\n",
    "\n",
    "print(f\"Proximidad del Nodo {vertice} en fecha {fecha} a otros grupos En DataFrame\")\n",
    "print(f\"Numerador: {W_jk_otros}\")\n",
    "print(f\"Denominador: {den}\")\n",
    "print(f\"Proximidad a Otros: {proximidad_otros}\")\n",
    "print(f\"Numerador/Denominador: {W_jk_otros/den}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run to load\n",
    "group_segregation_daily = pd.read_pickle(os.path.join(path,\"Segregation\",'group_segregation_daily.pkl'))\n",
    "global_segregation_daily = pd.read_pickle(os.path.join(path,\"Segregation\",'global_segregation_daily.pkl'))\n",
    "\n",
    "group_segregation_3day = pd.read_pickle(os.path.join(path,\"Segregation\",'group_segregation_3day.pkl'))\n",
    "global_segregation_3day = pd.read_pickle(os.path.join(path,\"Segregation\",'global_segregation_3day.pkl'))\n",
    "\n",
    "individual_group_segregation = pd.read_pickle(os.path.join(path,\"Segregation\",'individual_group_segregation.pkl'))\n",
    "individual_node_segregation = pd.read_pickle(os.path.join(path,\"Segregation\",'individual_node_segregation.pkl'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
