{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Concatenate tweets\n",
    "\n",
    "In this notebook, we will consolidate all the tweet files into a single compressed pickle file for further analysis. We have three main sets of data that we need to store: \n",
    "\n",
    "1. Data from January 2021.\n",
    "2. Data from October 2021.\n",
    "3. Data from April 28 to June 30.\n",
    "\n",
    "Each of these samples corresponds to a specific moment relevant for our analysis. The October data is used for analyzing our community during election periods, specifically the regional elections in Colombia that took place in October 2019. The data from January 2021 represents the period three months before the \"Paro Nacional,\" allowing us to track our community before the social outbreak. Finally, we have the data from the time of the \"Paro Nacional,\" which will be the focal point of our analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = r\"/mnt/disk2/Data/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regional elections: October 2019"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We create an empty aux list that will store the tweets.\n",
    "tweets_aux = []\n",
    "files_oct = glob(os.path.join(path, 'users_oct_19/*.csv'))\n",
    "\n",
    "for file in tqdm(files_oct):\n",
    "    tweets_aux.append(pd.read_csv(file))\n",
    "\n",
    "# Finally, the tweet dataframe is established and tweets_aux is deleted.  \n",
    "tweets = pd.concat(tweets_aux)\n",
    "del tweets_aux\n",
    "tweets = tweets.sort_values('ID').reset_index(drop = True)\n",
    "\n",
    "# Store results\n",
    "tweets.to_pickle(os.path.join(path, \"Tweets_DataFrames/tweets_oct19.gzip\"), compression = \"gzip\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Before Paro Nacional: January 2021"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We identify two users with their file corrupted: Usuario_82383620 and Usuario_2526574133"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We create an empty aux list that will store the tweets.\n",
    "tweets_aux = []\n",
    "files_jan = glob(os.path.join(path, \"RawData/users_jan/*.csv\"))\n",
    "\n",
    "for file in tqdm(files_jan):\n",
    "    tweets_aux.append(pd.read_csv(file))\n",
    "\n",
    "# Finally, the tweet dataframe is established and tweets_aux is deleted.  \n",
    "tweets_jan = pd.concat(tweets_aux)\n",
    "del tweets_aux\n",
    "tweets_jan = tweets_jan.sort_values('ID').reset_index(drop = True)\n",
    "\n",
    "# We check the DataFrame\n",
    "print('Shape: ',tweets_jan.shape)\n",
    "tweets_jan.head()\n",
    "\n",
    "# Store results\n",
    "# run sudo chmod 777 /mnt/disk2/Data/Tweets_DataFrames in bash if it is needed\n",
    "tweets_jan.to_pickle(os.path.join(path, \"Tweets_DataFrames/tweets_jan21.gzip\"), compression = \"gzip\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Paro Nacional: April 28 - June 30 2021"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "37324"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "files_v1 = glob(os.path.join(path, 'RawData/Usuarios_V1/*.csv'))\n",
    "len(files_v1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/mnt/disk2/Data/RawData/Usuarios_V1/Usuario_31172486-Juan’s MacBook Air.csv',\n",
       " '/mnt/disk2/Data/RawData/Usuarios_V1/Usuario_418406996-Juan’s MacBook Air.csv',\n",
       " '/mnt/disk2/Data/RawData/Usuarios_V1/Usuario_2564362444-Juan’s MacBook Air.csv',\n",
       " '/mnt/disk2/Data/RawData/Usuarios_V1/Usuario_721013234-Juan’s MacBook Air.csv',\n",
       " '/mnt/disk2/Data/RawData/Usuarios_V1/Usuario_286818396-Juan’s MacBook Air.csv',\n",
       " '/mnt/disk2/Data/RawData/Usuarios_V1/Usuario_186496554-Juan’s MacBook Air.csv',\n",
       " '/mnt/disk2/Data/RawData/Usuarios_V1/Usuario_799005686-Juan’s MacBook Air.csv',\n",
       " '/mnt/disk2/Data/RawData/Usuarios_V1/Usuario_3327640233-Juan’s MacBook Air.csv',\n",
       " '/mnt/disk2/Data/RawData/Usuarios_V1/Usuario_2183412805-Juan’s MacBook Air.csv',\n",
       " '/mnt/disk2/Data/RawData/Usuarios_V1/Usuario_1286016270517841931-Juan’s MacBook Air.csv',\n",
       " '/mnt/disk2/Data/RawData/Usuarios_V1/Usuario_820659585770016769-Juan’s MacBook Air.csv',\n",
       " '/mnt/disk2/Data/RawData/Usuarios_V1/Usuario_180601646-Juan’s MacBook Air.csv',\n",
       " '/mnt/disk2/Data/RawData/Usuarios_V1/Usuario_1239743072516411394-Juan’s MacBook Air.csv',\n",
       " '/mnt/disk2/Data/RawData/Usuarios_V1/Usuario_4861663593-Juan’s MacBook Air.csv',\n",
       " '/mnt/disk2/Data/RawData/Usuarios_V1/Usuario_328178806-Juan’s MacBook Air.csv',\n",
       " '/mnt/disk2/Data/RawData/Usuarios_V1/Usuario_297706813-Juan’s MacBook Air.csv',\n",
       " '/mnt/disk2/Data/RawData/Usuarios_V1/Usuario_909410714-Juan’s MacBook Air.csv']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_list = []\n",
    "users_information = []\n",
    "\n",
    "# cols = ['ID', 'Author ID', 'Author Name', 'Date', 'Text', 'Replies', 'Retweets', 'Favorites', 'Quotes', 'is Retweet?',\n",
    "#            'Reply To User Name', 'Mentions', 'Referenced Tweet', 'Reference Type', 'Referenced Tweet Author ID']\n",
    "\n",
    "problems = []\n",
    "\n",
    "def unique_to_string(x):\n",
    "    unique_values = x.unique()\n",
    "    return ', '.join(map(str, unique_values))\n",
    "\n",
    "# Counter and variable for keeping track of file names\n",
    "count = 0 # Amount of Tweets\n",
    "n = 0 # Number of Cheackpoint\n",
    "\n",
    "# Runtime 1 Hour!!!!!\n",
    "for file in tqdm(files_v1):\n",
    "    try:\n",
    "        # df = pd.read_csv(file, usecols = cols)\n",
    "        df = pd.read_csv(file)\n",
    "        # Fix some datatypes\n",
    "        df[['Author Followers', 'Author Following', 'Author Tweets']] = df[['Author Followers', 'Author Following', 'Author Tweets']].map(lambda x: pd.to_numeric(x, errors = 'coerce'))\n",
    "        df_list.append(df)\n",
    "        count += len(df)\n",
    "\n",
    "        # Save user information\n",
    "        user_information = df.groupby(['Author ID', 'Author Name']).agg({\n",
    "                'Author Location': unique_to_string,\n",
    "                'Author Description': unique_to_string,\n",
    "                'Author Followers': lambda x: np.nanmean(x),\n",
    "                'Author Following': lambda x: np.nanmean(x),\n",
    "                'Author Tweets': lambda x: np.nanmax(x),\n",
    "                'Author Verified': unique_to_string})\n",
    "        \n",
    "        users_information.append(user_information)\n",
    "        \n",
    "        # If we reach or exceed 10 million rows, save the file and reset\n",
    "        if count >= 10_000_000:\n",
    "            n += 1\n",
    "            concat_df = pd.concat(df_list)\n",
    "            output_filename = f\"tweets_paro_{n}.gzip\"\n",
    "            concat_df.to_pickle(os.path.join(path, f\"Tweets_DataFrames/{output_filename}\"), compression='gzip')\n",
    "            \n",
    "            # Reset counter and list\n",
    "            count = 0\n",
    "            df_list = []\n",
    "            \n",
    "    except (ValueError, KeyError) as e:\n",
    "        problems.append(file)\n",
    "\n",
    "problems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save any remaining data after the loop\n",
    "# Runtime 5 minutes\n",
    "if df_list:\n",
    "    n += 1\n",
    "    concat_df = pd.concat(df_list)\n",
    "    output_filename = f\"tweets_paro_{n}.gzip\"\n",
    "    # If necessary, run \"sudo chmod 777 Data/Tweets_DataFrames\" in bash\n",
    "    concat_df.to_pickle(os.path.join(path, f\"Tweets_DataFrames/{output_filename}\"), compression = 'gzip')\n",
    "\n",
    "del df_list, concat_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1710478/1078093390.py:5: RuntimeWarning: Mean of empty slice\n",
      "  'Author Followers': lambda x: np.nanmean(x),\n",
      "/tmp/ipykernel_1710478/1078093390.py:6: RuntimeWarning: Mean of empty slice\n",
      "  'Author Following': lambda x: np.nanmean(x),\n",
      "/tmp/ipykernel_1710478/1078093390.py:7: RuntimeWarning: All-NaN axis encountered\n",
      "  'Author Tweets': lambda x: np.nanmax(x),\n"
     ]
    }
   ],
   "source": [
    "# runtime 22 minutes\n",
    "concat_users_information = pd.concat(users_information)\n",
    "concat_users_information = concat_users_information.groupby(['Author ID', 'Author Name']) \\\n",
    "    .agg({'Author Location': unique_to_string,\n",
    "            'Author Description': unique_to_string,\n",
    "            'Author Followers': lambda x: np.nanmean(x),\n",
    "            'Author Following': lambda x: np.nanmean(x),\n",
    "            'Author Tweets': lambda x: np.nanmax(x),\n",
    "            'Author Verified': unique_to_string})\n",
    "concat_users_information.to_pickle(os.path.join(path, \"Tweets_DataFrames/users_information.gzip\"), \n",
    "                                   compression = 'gzip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'concat_users_information' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/mnt/disk2/fcastrillon/Analysis-of-Tweets-During-the-2021-Social-Unrest/Code/1_Concatenate_Tweets.ipynb Cell 14\u001b[0m line \u001b[0;36m2\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B157.253.242.110/mnt/disk2/fcastrillon/Analysis-of-Tweets-During-the-2021-Social-Unrest/Code/1_Concatenate_Tweets.ipynb#X16sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# We should correct this\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2B157.253.242.110/mnt/disk2/fcastrillon/Analysis-of-Tweets-During-the-2021-Social-Unrest/Code/1_Concatenate_Tweets.ipynb#X16sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m concat_users_information\n",
      "\u001b[0;31mNameError\u001b[0m: name 'concat_users_information' is not defined"
     ]
    }
   ],
   "source": [
    "# We should correct this\n",
    "concat_users_information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [03:03<00:00, 36.65s/it]\n"
     ]
    }
   ],
   "source": [
    "tweets_paro = glob('/mnt/disk2/Data/Tweets_DataFrames/tweets_paro_*')\n",
    "\n",
    "tweets = pd.DataFrame()\n",
    "for file in tqdm(tweets_paro):\n",
    "    tweets_df = pd.read_pickle(file, compression = \"gzip\")\n",
    "\n",
    "    tweets = pd.concat([tweets, tweets_df], axis = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tweets Lite\n",
    "We create a reduced version of the Paro data frame. This will have the same amount of rows but we will only store four columns: 'Author ID', 'Date', 'Reference Type', 'Referenced Tweet Author ID'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get just the columns that we need for the Graph construction\n",
    "cols = ['Author ID','Author Name', 'Date', 'Reference Type', 'Referenced Tweet Author ID']\n",
    "tweets_lite = tweets[cols].reset_index(drop = True)\n",
    "# Store results\n",
    "# run sudo chmod 777 Data/Tweets_DataFrames in bash if it is needed\n",
    "tweets_lite.to_pickle(os.path.join(path, \"Tweets_DataFrames/tweets_lite.gzip\"), compression = \"gzip\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outoputs\n",
    "\n",
    "The output of this Notebook are stored \"/mnt/disk2/Data/Tweets_DataFrames\" and are listed below:\n",
    "\n",
    "- **tweets_jan21.gzip**: Dataframe for the Tweets for our users during January of 2021. 3 Months before the Paro\n",
    "- **tweets_oct19.gzip**: Dataframe for the Tweets for our users during October of 2019. Regional elections Period\n",
    "- **tweets_paro_i.gzip**: 5 dataframes for the tweets of our users between April 28 to June 30 of 2021\n",
    "- **tweets_lite.pkl**: Lite version of **tweets_Usuarios_V1.gzip** that contains just the colmns needed for the graph construction. Which is Author ID, Reference Type, Date and Retweet Author"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
