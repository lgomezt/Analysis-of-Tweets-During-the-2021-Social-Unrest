{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning\n",
    "\n",
    "In this notebook we will dump all the tweets file into a consolidated pickle file compress so that we can load it there for further analysis. We Have 3 Main samples of Data which we have to store. First, We have Data from January 2021; Second, October 2021; and finally, From April 28 to June 30.\n",
    "\n",
    "Each of this samples corresponds to a Particular moment relevant for the analysis. Octaber data is used for the anaylisis of our community during election periods. October 2019 is the month of the regional elections in Colombia. January 2021 corresponds is 3 months before the so called \"Paro Nacional\". This can let us track aour community before the social out break. Finally we have the time of the \"Paro Nacional\" Which will be the center of our analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Matrices',\n",
       " 'nodes.csv',\n",
       " 'Pickle',\n",
       " 'Usuarios_V1',\n",
       " 'Graphs',\n",
       " 'users_jan',\n",
       " 'Tweets_DataFrames']"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = r\"/mnt/disk2/Data/\"\n",
    "os.listdir(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Concatenate Users\n",
    "\n",
    "### 1.1. January 2021"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We identify two users with their file corrupted: Usuario_82383620 and Usuario_2526574133"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We create an empty aux list that will store the tweets.\n",
    "tweets_aux = []\n",
    "files_jan = glob(os.path.join(path, \"users_jan/*.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files_jan = glob(os.path.join(path, \"users_jan/*.csv\"))\n",
    "\n",
    "for file in tqdm(files_jan):\n",
    "    tweets_aux.append(pd.read_csv(file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finally, the tweet dataframe is established and tweets_aux is deleted.  \n",
    "tweets = pd.concat(tweets_aux)\n",
    "del tweets_aux\n",
    "tweets = tweets.sort_values('ID').reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store results\n",
    "tweets.to_pickle(os.path.join(path, \"users_jan/tweets_jan21.gzip\"), compression = \"gzip\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. October 2021"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We create an empty aux list that will store the tweets.\n",
    "tweets_aux = []\n",
    "files_oct = glob(os.path.join(path, 'users_oct_19/*.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in tqdm(files_oct):\n",
    "    tweets_aux.append(pd.read_csv(file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finally, the tweet dataframe is established and tweets_aux is deleted.  \n",
    "tweets = pd.concat(tweets_aux)\n",
    "del tweets_aux\n",
    "tweets = tweets.sort_values('ID').reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store results\n",
    "tweets.to_pickle(os.path.join(path, \"users_oct_19/tweets_oct19.gzip\"), compression = \"gzip\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3. Paro Nacional: April 28 - June 30 2021"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section of the Notebook we will create the dataframe for all the tweets between **April 28 of 2021** to **June 30 of 2021**. For this task, the dataframe we eant to construct is made from 28 pieces that are saved as ```.pkl``` files in the Data/Tweets_Dataframes/Tweets Folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We create an empty aux list that will store the tweets.\n",
    "tweets_aux = []\n",
    "\n",
    "files_v1 = glob(os.path.join(path, 'Usuarios_V1/*.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▏        | 4429/37324 [01:07<06:10, 88.80it/s] "
     ]
    }
   ],
   "source": [
    "for file in tqdm(files_v1):\n",
    "    tweets_aux.append(pd.read_csv(file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We check the correct concatenation of the the Data Frame.\n",
    "print(tweets.shape)\n",
    "tweets.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finally, the tweet dataframe is established and tweets_aux is deleted.  \n",
    "tweets = pd.concat(tweets_aux)\n",
    "del tweets_aux\n",
    "# tweets = tweets.sort_values('ID').reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get just the columns that we need\n",
    "cols = ['Author ID',\n",
    "        'Date',\n",
    "        'Reference Type',\n",
    "        'Referenced Tweet Author ID']\n",
    "tweets_lite = tweets[cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store results\n",
    "# run sudo chmod 777 Data/Tweets_DataFrames in bash if it is needed\n",
    "tweets.to_pickle(os.path.join(path, \"Tweets_DataFrames/tweets_Usuarios_V1.gzip\"), compression = \"gzip\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Tweets Lite\n",
    "\n",
    "In this section, we will just import the columns tahta are neccesarry for constructing the graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get just the columns that we need\n",
    "cols = ['Author ID',\n",
    "        'Date',\n",
    "        'Reference Type',\n",
    "        'Referenced Tweet Author ID']\n",
    "\n",
    "# We create an empty aux list that will store the tweets.\n",
    "tweets_lite_aux = []\n",
    "ID = []\n",
    "files = glob('/mnt/disk2/Data/Usuarios_V1/*.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The files location for this Dataframe is in the OneDrive folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "44"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "XD = pd.read_csv(files[127])\n",
    "files[127].find('-Juan’s MacBook Air')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['dkaodnqw']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'dkaodnqw'.split('-')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "127\n",
      "3866\n",
      "7745\n",
      "8516\n",
      "9004\n",
      "10781\n",
      "11069\n",
      "14674\n",
      "16656\n",
      "18071\n",
      "18764\n",
      "19846\n",
      "28578\n",
      "30812\n",
      "33635\n",
      "35352\n",
      "37202\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "for index, f in enumerate(files):\n",
    "    x = f.split(\"/\")[5]\n",
    "    if x.find('-Juan’s MacBook Air')==-1:\n",
    "        pass\n",
    "    else:\n",
    "        print(index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 127/37324 [00:01<05:43, 108.35it/s]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Usecols do not match columns, columns expected but not found: ['Referenced Tweet Author ID', 'Date', 'Reference Type', 'Author ID']",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/mnt/disk2/fcastrillon/Analysis-of-Tweets-During-the-2021-Social-Unrest/Code/Save_tweets.ipynb Cell 28\u001b[0m line \u001b[0;36m3\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B157.253.242.110/mnt/disk2/fcastrillon/Analysis-of-Tweets-During-the-2021-Social-Unrest/Code/Save_tweets.ipynb#X14sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# We store the tweets in the list.\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B157.253.242.110/mnt/disk2/fcastrillon/Analysis-of-Tweets-During-the-2021-Social-Unrest/Code/Save_tweets.ipynb#X14sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mfor\u001b[39;00m file \u001b[39min\u001b[39;00m tqdm(files):\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2B157.253.242.110/mnt/disk2/fcastrillon/Analysis-of-Tweets-During-the-2021-Social-Unrest/Code/Save_tweets.ipynb#X14sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m     tweets_lite_aux\u001b[39m.\u001b[39mappend(pd\u001b[39m.\u001b[39;49mread_csv(file, usecols \u001b[39m=\u001b[39;49m cols))\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B157.253.242.110/mnt/disk2/fcastrillon/Analysis-of-Tweets-During-the-2021-Social-Unrest/Code/Save_tweets.ipynb#X14sdnNjb2RlLXJlbW90ZQ%3D%3D?line=4'>5</a>\u001b[0m     \u001b[39m# Extract the XXXXX part by splitting the filename at the underscore and taking the second part\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B157.253.242.110/mnt/disk2/fcastrillon/Analysis-of-Tweets-During-the-2021-Social-Unrest/Code/Save_tweets.ipynb#X14sdnNjb2RlLXJlbW90ZQ%3D%3D?line=5'>6</a>\u001b[0m     number \u001b[39m=\u001b[39m file\u001b[39m.\u001b[39msplit(\u001b[39m\"\u001b[39m\u001b[39m/\u001b[39m\u001b[39m\"\u001b[39m)[\u001b[39m5\u001b[39m]\n",
      "File \u001b[0;32m~/.conda/envs/gt/lib/python3.11/site-packages/pandas/io/parsers/readers.py:948\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m    935\u001b[0m kwds_defaults \u001b[39m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m    936\u001b[0m     dialect,\n\u001b[1;32m    937\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    944\u001b[0m     dtype_backend\u001b[39m=\u001b[39mdtype_backend,\n\u001b[1;32m    945\u001b[0m )\n\u001b[1;32m    946\u001b[0m kwds\u001b[39m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m--> 948\u001b[0m \u001b[39mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[0;32m~/.conda/envs/gt/lib/python3.11/site-packages/pandas/io/parsers/readers.py:611\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    608\u001b[0m _validate_names(kwds\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mnames\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m))\n\u001b[1;32m    610\u001b[0m \u001b[39m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 611\u001b[0m parser \u001b[39m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\n\u001b[1;32m    613\u001b[0m \u001b[39mif\u001b[39;00m chunksize \u001b[39mor\u001b[39;00m iterator:\n\u001b[1;32m    614\u001b[0m     \u001b[39mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m~/.conda/envs/gt/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1448\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1445\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptions[\u001b[39m\"\u001b[39m\u001b[39mhas_index_names\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m kwds[\u001b[39m\"\u001b[39m\u001b[39mhas_index_names\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m   1447\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles: IOHandles \u001b[39m|\u001b[39m \u001b[39mNone\u001b[39;00m \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m-> 1448\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_engine \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_make_engine(f, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mengine)\n",
      "File \u001b[0;32m~/.conda/envs/gt/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1723\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1720\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(msg)\n\u001b[1;32m   1722\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1723\u001b[0m     \u001b[39mreturn\u001b[39;00m mapping[engine](f, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions)\n\u001b[1;32m   1724\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m:\n\u001b[1;32m   1725\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/.conda/envs/gt/lib/python3.11/site-packages/pandas/io/parsers/c_parser_wrapper.py:140\u001b[0m, in \u001b[0;36mCParserWrapper.__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m    136\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39morig_names \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    137\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39musecols_dtype \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mstring\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mset\u001b[39m(usecols)\u001b[39m.\u001b[39missubset(\n\u001b[1;32m    138\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39morig_names\n\u001b[1;32m    139\u001b[0m ):\n\u001b[0;32m--> 140\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_validate_usecols_names(usecols, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49morig_names)\n\u001b[1;32m    142\u001b[0m \u001b[39m# error: Cannot determine type of 'names'\u001b[39;00m\n\u001b[1;32m    143\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnames) \u001b[39m>\u001b[39m \u001b[39mlen\u001b[39m(usecols):  \u001b[39m# type: ignore[has-type]\u001b[39;00m\n\u001b[1;32m    144\u001b[0m     \u001b[39m# error: Cannot determine type of 'names'\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/gt/lib/python3.11/site-packages/pandas/io/parsers/base_parser.py:969\u001b[0m, in \u001b[0;36mParserBase._validate_usecols_names\u001b[0;34m(self, usecols, names)\u001b[0m\n\u001b[1;32m    967\u001b[0m missing \u001b[39m=\u001b[39m [c \u001b[39mfor\u001b[39;00m c \u001b[39min\u001b[39;00m usecols \u001b[39mif\u001b[39;00m c \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m names]\n\u001b[1;32m    968\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(missing) \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m--> 969\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    970\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mUsecols do not match columns, columns expected but not found: \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    971\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mmissing\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    972\u001b[0m     )\n\u001b[1;32m    974\u001b[0m \u001b[39mreturn\u001b[39;00m usecols\n",
      "\u001b[0;31mValueError\u001b[0m: Usecols do not match columns, columns expected but not found: ['Referenced Tweet Author ID', 'Date', 'Reference Type', 'Author ID']"
     ]
    }
   ],
   "source": [
    "# We store the tweets in the list.\n",
    "for file in tqdm(files):\n",
    "    tweets_lite_aux.append(pd.read_csv(file, usecols = cols))\n",
    "\n",
    "    # Extract the XXXXX part by splitting the filename at the underscore and taking the second part\n",
    "    number = file.split(\"/\")[5]\n",
    "    \n",
    "    # Remove the file extension if present\n",
    "    number = number.split('.')[0].split('_')[1].split('-')[0]\n",
    "    ID.append(number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We concat the list and then deleted\n",
    "tweets_lite = pd.concat(tweets_lite_aux)\n",
    "del tweets_lite_aux"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we add the ID to the Dataframe\n",
    "ID = set(ID)\n",
    "ID = {int(element) for element in ID}\n",
    "minimum = min(ID)\n",
    "tweets_lite = tweets_lite[tweets_lite['Author ID'] >= minimum]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Conclusión\n",
    "\n",
    "The output of this Notebook is listed Below\n",
    "\n",
    "- **tweets_jan21.gzip**: Dataframe for the Tweets for our users during January of 2021. 3 Months before the social estallido\n",
    "\n",
    "- **tweets_oct19.gzip**: Dataframe for the Tweets for our users during October of 2019. Regional elections Period\n",
    "\n",
    "- **tweets_Usuarios_V1.gzip**: Dataframe for the tweets of our users between April 28 to June 30 of 2021\n",
    "\n",
    "- **tweets_lite.pkl**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
