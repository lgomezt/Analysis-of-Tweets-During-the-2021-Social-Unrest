{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning\n",
    "\n",
    "In this notebook we will dump all the tweets file into a consolidated pickle file compress so that we can load it there for further analysis. We Have 3 Main samples of Data which we have to store. First, We have Data from January 2021; Second, October 2021; and finally, From April 28 to June 30.\n",
    "\n",
    "Each of this samples corresponds to a Particular moment relevant for the analysis. Octaber data is used for the anaylisis of our community during election periods. October 2019 is the month of the regional elections in Colombia. January 2021 corresponds is 3 months before the so called \"Paro Nacional\". This can let us track aour community before the social out break. Finally we have the time of the \"Paro Nacional\" Which will be the center of our analysis.\n",
    "\n",
    "This Notebook has 3 Chapters.\n",
    "1. Concatenate Users:\n",
    "\n",
    "    1.1. January 2021\n",
    "\n",
    "    1.2. October 2019\n",
    "\n",
    "    1.3. Paro Nacional\n",
    "\n",
    "    1.4. Tweets Lite\n",
    "    \n",
    "2. Save Results\n",
    "3. Conclusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Matrices_DayToDay',\n",
       " 'XD',\n",
       " 'Graphs_2',\n",
       " 'Matrices',\n",
       " 'nodes.csv',\n",
       " 'RawData',\n",
       " 'Pickle',\n",
       " 'Graphs',\n",
       " 'Tweets_DataFrames']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = r\"/mnt/disk2/Data/\"\n",
    "os.listdir(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Concatenate Users\n",
    "\n",
    "### 1.1. January 2021"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We identify two users with their file corrupted: Usuario_82383620 and Usuario_2526574133"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We create an empty aux list that will store the tweets.\n",
    "tweets_aux = []\n",
    "files_jan = glob(os.path.join(path, \"users_jan/*.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files_jan = glob(os.path.join(path, \"users_jan/*.csv\"))\n",
    "\n",
    "for file in tqdm(files_jan):\n",
    "    tweets_aux.append(pd.read_csv(file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finally, the tweet dataframe is established and tweets_aux is deleted.  \n",
    "tweets = pd.concat(tweets_aux)\n",
    "del tweets_aux\n",
    "tweets = tweets.sort_values('ID').reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store results\n",
    "tweets.to_pickle(os.path.join(path, \"users_jan/tweets_jan21.gzip\"), compression = \"gzip\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. October 2021"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We create an empty aux list that will store the tweets.\n",
    "tweets_aux = []\n",
    "files_oct = glob(os.path.join(path, 'users_oct_19/*.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in tqdm(files_oct):\n",
    "    tweets_aux.append(pd.read_csv(file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finally, the tweet dataframe is established and tweets_aux is deleted.  \n",
    "tweets = pd.concat(tweets_aux)\n",
    "del tweets_aux\n",
    "tweets = tweets.sort_values('ID').reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store results\n",
    "tweets.to_pickle(os.path.join(path, \"users_oct_19/tweets_oct19.gzip\"), compression = \"gzip\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3. Paro Nacional: April 28 - June 30 2021"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section of the Notebook we will create the dataframe for all the tweets between **April 28 of 2021** to **June 30 of 2021**. For this task, the dataframe we eant to construct is made from 28 pieces that are saved as ```.pkl``` files in the Data/Tweets_Dataframes/Tweets Folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We create an empty aux list that will store the tweets.\n",
    "tweets_aux = []\n",
    "\n",
    "files_v1 = glob(os.path.join(path, 'RawData/Usuarios_V1/*.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "37324"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(files_v1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 43%|████▎     | 15937/37324 [04:08<04:07, 86.36it/s] /tmp/ipykernel_740251/3203433731.py:3: DtypeWarning: Columns (6,8,10,17) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  tweets_aux.append(pd.read_csv(file))\n",
      " 47%|████▋     | 17645/37324 [04:35<04:20, 75.51it/s] /tmp/ipykernel_740251/3203433731.py:3: DtypeWarning: Columns (18) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  tweets_aux.append(pd.read_csv(file))\n",
      " 54%|█████▎    | 20034/37324 [05:13<05:08, 56.08it/s] /tmp/ipykernel_740251/3203433731.py:3: DtypeWarning: Columns (6,8,10,17) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  tweets_aux.append(pd.read_csv(file))\n",
      "100%|██████████| 37324/37324 [09:28<00:00, 65.62it/s] \n"
     ]
    }
   ],
   "source": [
    "# Run time: 9 minutes aprox\n",
    "for file in tqdm(files_v1):\n",
    "    tweets_aux.append(pd.read_csv(file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Finally, the tweet dataframe is established and tweets_aux is deleted.\n",
    "# Run time: 50 sec. aprox.  \n",
    "tweets = pd.concat(tweets_aux)\n",
    "del tweets_aux\n",
    "# tweets = tweets.sort_values('ID').reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We check the correct concatenation of the the Data Frame.\n",
    "print(tweets.shape)\n",
    "tweets.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store results\n",
    "# run sudo chmod 777 Data/Tweets_DataFrames in bash if it is needed\n",
    "tweets.to_pickle(os.path.join(path, \"Tweets_DataFrames/tweets_Usuarios_V1.gzip\"), compression = \"gzip\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1. Tweets Lite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get just the columns that we need for the Graph construction\n",
    "cols = ['Author ID',\n",
    "        'Date',\n",
    "        'Reference Type',\n",
    "        'Referenced Tweet Author ID']\n",
    "tweets_lite = tweets[cols]\n",
    "tweets_lite"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store results\n",
    "# run sudo chmod 777 Data/Tweets_DataFrames in bash if it is needed\n",
    "tweets_lite.to_pickle(os.path.join(path, \"Tweets_DataFrames/tweets_lite.gzip\"), compression = \"gzip\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Conclusion\n",
    "\n",
    "The output of this Notebook is listed Below\n",
    "\n",
    "- **tweets_jan21.gzip**: Dataframe for the Tweets for our users during January of 2021. 3 Months before the social estallido\n",
    "\n",
    "- **tweets_oct19.gzip**: Dataframe for the Tweets for our users during October of 2019. Regional elections Period\n",
    "\n",
    "- **tweets_Usuarios_V1.gzip**: Dataframe for the tweets of our users between April 28 to June 30 of 2021\n",
    "\n",
    "- **tweets_lite.pkl**: Lite version of **tweets_Usuarios_V1.gzip** that contains just the colmns needed for the graph construction. Which is Author ID, Reference Type, Date and Retweet Author"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
