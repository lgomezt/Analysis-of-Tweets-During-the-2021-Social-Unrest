{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data cleaning\n",
    "##  1. Concatenate Users\n",
    "We have two important samples. One from January of 2021 and other for October of 2019. This script imports the rts made by each user in our sample, one by one, and later combine all the retweets in a pickle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = r\"C:\\Users\\Lucas\\OneDrive - Universidad de los Andes\\Economía Uniandes\\Asistencia de investigacion\\TorniquetesLight\\Twitter\\Data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['.ipynb_checkpoints',\n",
       " 'Checks',\n",
       " 'graph_v1',\n",
       " 'Keywords',\n",
       " 'useful_dicts',\n",
       " 'users_jan',\n",
       " 'users_oct_19',\n",
       " 'Usuarios',\n",
       " 'Usuarios_V1']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import January 21' tweets\n",
    "[comment]: <> (Duda #1. En el archivo original el path es \"../../Data/Checks/28-06-23\" pero ahí solo hay .pkl)\n",
    "\n",
    "We identify two users with their file corrupted: Usuario_82383620 and Usuario_2526574133"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We create an empty aux list that will store the tweets.\n",
    "tweets_aux = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files_jan = glob.glob(os.path.join(path, \"users_jan/*.csv\"))\n",
    "\n",
    "for file in tqdm(files_jan):\n",
    "    tweets_aux.append(pd.read_csv(file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finally, the tweet dataframe is established and tweets_aux is deleted.  \n",
    "tweets = pd.concat(tweets_aux)\n",
    "del tweets_aux\n",
    "tweets = tweets.sort_values('ID').reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store results\n",
    "tweets.to_pickle(os.path.join(path, \"users_jan/tweets_jan21.gzip\"), compression = \"gzip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import October 19' tweets\n",
    "\n",
    "We identify one user with their file corrupted: Usuario_1153335229597278208"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We create an empty aux list that will store the tweets.\n",
    "tweets_aux = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files_oct = glob.glob(os.path.join(path, 'users_oct_19/*.csv'))\n",
    "\n",
    "for file in tqdm(files_oct):\n",
    "    tweets_aux.append(pd.read_csv(file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finally, the tweet dataframe is established and tweets_aux is deleted.  \n",
    "tweets = pd.concat(tweets_aux)\n",
    "del tweets_aux\n",
    "tweets = tweets.sort_values('ID').reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store results\n",
    "tweets.to_pickle(os.path.join(path, \"users_oct_19/tweets_oct19.gzip\"), compression = \"gzip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Usuarios_V1 tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We create an empty aux list that will store the tweets.\n",
    "tweets_aux = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 31%|███       | 11427/37324 [03:36<08:32, 50.54it/s]C:\\Users\\Lucas\\AppData\\Local\\Temp\\ipykernel_12752\\1028835383.py:4: DtypeWarning: Columns (18) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  tweets_aux.append(pd.read_csv(file))\n",
      " 44%|████▎     | 16313/37324 [05:05<04:59, 70.19it/s]C:\\Users\\Lucas\\AppData\\Local\\Temp\\ipykernel_12752\\1028835383.py:4: DtypeWarning: Columns (6,8,10,17) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  tweets_aux.append(pd.read_csv(file))\n",
      " 73%|███████▎  | 27146/37324 [08:45<02:28, 68.49it/s]  C:\\Users\\Lucas\\AppData\\Local\\Temp\\ipykernel_12752\\1028835383.py:4: DtypeWarning: Columns (6,8,10,17) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  tweets_aux.append(pd.read_csv(file))\n",
      "100%|██████████| 37324/37324 [16:17<00:00, 38.18it/s]  \n"
     ]
    }
   ],
   "source": [
    "files_v1 = glob.glob(os.path.join(path, 'Usuarios_V1/*.csv'))\n",
    "\n",
    "for file in tqdm(files_v1):\n",
    "    tweets_aux.append(pd.read_csv(file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finally, the tweet dataframe is established and tweets_aux is deleted.  \n",
    "tweets = pd.concat(tweets_aux)\n",
    "del tweets_aux\n",
    "# tweets = tweets.sort_values('ID').reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store results\n",
    "tweets.to_pickle(os.path.join(path, \"Usuarios_V1/tweets_Usuarios_V1.gzip\"), compression = \"gzip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Assign political affiliation based on retweets\n",
    "This section determines the political affiliation (left, center, right) of each user in our sample by analyzing the retweets they've made.\n",
    "\n",
    "We use a list of political influencers that have been previously categorized as left, center, or right by La Silla Vacia, a Colombian news outlet. For each user, we tally the number of retweets (excluding those with comments) corresponding to each influencer. From this, we calculate the total number of tweets associated with each political category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's load the id_to_party_map pickle file. This pickle contains some labels for political influencers in Colombia. \n",
    "# These were assigned using \"La Silla Vacia\" research.\n",
    "mapa = pd.read_pickle(\"../Data/mapa.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we assign each RT a political label according to its influencer's label.\n",
    "tweets[\"Party\"] = np.nan\n",
    "tweets.loc[tweets[\"Reference Type\"] == \"retweeted\", \"Party\"] = tweets.loc[tweets[\"Reference Type\"] == \"retweeted\",\n",
    "                                                                         \"Referenced Tweet Author ID\"].map(mapa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store results\n",
    "tweets.to_pickle(\"../Data/tweets_oct19_jan21.gzip\", compression = \"gzip\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
